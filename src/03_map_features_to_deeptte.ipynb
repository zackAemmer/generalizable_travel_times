{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import json\n",
    "\n",
    "from database import data_utils\n",
    "\n",
    "import contextily as cx\n",
    "import geopandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "feature_names = ['trip_id','file','locationtime','lat','lon','vehicle_id']\n",
    "data_types = ['object','object','int','float','float','object']\n",
    "feature_lookup = dict(zip(feature_names, data_types))\n",
    "\n",
    "\n",
    "import importlib\n",
    "importlib.reload(data_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get traces from all data (KCM)\n",
    "given_names = ['tripid','file','locationtime','lat','lon','vehicleid']\n",
    "train_data, train_fail_dates = data_utils.combine_specific_folder_data(\"../data/kcm_all\", data_utils.get_date_list(\"2022_11_01\", 14), given_names, feature_lookup)\n",
    "test_data, test_fail_dates = data_utils.combine_specific_folder_data(\"../data/kcm_all\", data_utils.get_date_list(\"2022_11_15\", 7), given_names, feature_lookup)\n",
    "print(train_fail_dates)\n",
    "print(test_fail_dates)\n",
    "\n",
    "# Calculate distance between points in each trajectory, do some filtering on speed, n_points\n",
    "train_traces = data_utils.calculate_trace_df(train_data, 'America/Los_Angeles')\n",
    "test_traces = data_utils.calculate_trace_df(test_data, 'America/Los_Angeles')\n",
    "\n",
    "# Match trajectories to timetables and do filtering on stop distance, availability\n",
    "train_traces = data_utils.clean_trace_df_w_timetables(train_traces, '../data/kcm_gtfs/2022_09_19/')\n",
    "test_traces = data_utils.clean_trace_df_w_timetables(test_traces, '../data/kcm_gtfs/2022_09_19/')\n",
    "\n",
    "# Save trace data for results analysis\n",
    "with open(\"../results/kcm2weeks/data/train_traces.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_traces, f)\n",
    "with open(\"../results/kcm2weeks/data/test_traces.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_traces, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get traces from all data (ATB)\n",
    "# # For now, we can use Norway dates that are post-2022_11_02\n",
    "# # Need to get mapping of old IDs to new IDs in order to use schedule data from prior to that date\n",
    "# # These dates are also somewhat low on data compared to previous\n",
    "# train_dates = [\n",
    "#     \"2022_11_02.pkl\",\n",
    "#     \"2022_11_03.pkl\",\n",
    "#     \"2022_11_06.pkl\",\n",
    "#     \"2022_11_07.pkl\",\n",
    "#     \"2022_11_08.pkl\",\n",
    "#     \"2022_11_09.pkl\",\n",
    "#     \"2022_11_10.pkl\",\n",
    "#     \"2022_11_13.pkl\",\n",
    "#     \"2022_11_14.pkl\",\n",
    "#     \"2022_11_15.pkl\",\n",
    "#     \"2022_11_16.pkl\",\n",
    "#     \"2022_11_17.pkl\",\n",
    "#     \"2022_11_20.pkl\",\n",
    "#     \"2022_11_21.pkl\"\n",
    "# ]\n",
    "# test_dates = [\n",
    "#     \"2022_11_22.pkl\",\n",
    "#     \"2022_11_23.pkl\",\n",
    "#     \"2022_11_24.pkl\",\n",
    "#     \"2022_11_25.pkl\",\n",
    "#     \"2022_11_26.pkl\",\n",
    "#     \"2022_11_27.pkl\",\n",
    "#     \"2022_11_28.pkl\"\n",
    "# ]\n",
    "# given_names = ['datedvehiclejourney','file','locationtime','lat','lon','vehicle']\n",
    "# train_data, train_fail_dates = data_utils.combine_specific_folder_data(\"../data/nwy_all\", train_dates, given_names, feature_lookup)\n",
    "# test_data, test_fail_dates = data_utils.combine_specific_folder_data(\"../data/nwy_all\", test_dates, given_names, feature_lookup)\n",
    "# print(train_fail_dates)\n",
    "# print(test_fail_dates)\n",
    "\n",
    "# # Calculate distance between points in each trajectory, do some filtering on speed\n",
    "# train_traces = data_utils.calculate_trace_df(train_data, 'Europe/Oslo')\n",
    "# test_traces = data_utils.calculate_trace_df(test_data, 'Europe/Oslo')\n",
    "\n",
    "# # Match trajectories to timetables and do filtering on stop distance, availability\n",
    "# train_traces = data_utils.clean_trace_df_w_timetables(train_traces, '../data/nwy_gtfs/2022_12_01/')\n",
    "# test_traces = data_utils.clean_trace_df_w_timetables(test_traces, '../data/nwy_gtfs/2022_12_01/')\n",
    "\n",
    "# # Save trace data for results analysis\n",
    "# with open(\"../results/nwy2weeks/data/train_traces.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(train_traces, f)\n",
    "# with open(\"../results/nwy2weeks/data/test_traces.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(test_traces, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vehicle_ids = pd.concat([train_traces['vehicle_id'], test_traces['vehicle_id']]).values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode vehicle ids to start from 0\n",
    "mapping = {v:k for k,v in enumerate(set(all_vehicle_ids))}\n",
    "train_recode = [mapping[y] for y in train_traces['vehicle_id'].values.flatten()]\n",
    "test_recode = [mapping[y] for y in test_traces['vehicle_id'].values.flatten()]\n",
    "\n",
    "train_traces['vehicle_id_recode'] = train_recode\n",
    "test_traces['vehicle_id_recode'] = test_recode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This must be put into models/base/Attr.py\n",
    "# It is possible that not all vehicle ids are in the training data\n",
    "len(pd.unique(all_vehicle_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_files = 5\n",
    "# If there is a minibatch of 1, it will crash DeepTTE. So we just drop a sample if true.\n",
    "if len(test_traces) % batch_size == 1:\n",
    "    test_traces = test_traces.iloc[0:len(test_traces)-1,:]\n",
    "\n",
    "# Just drop last minibatch for train data that is split across files\n",
    "extras = len(train_traces) % (batch_size * num_files)\n",
    "train_traces = train_traces.iloc[0:len(train_traces)-extras,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_traces_dict = data_utils.map_to_deeptte(train_traces)\n",
    "test_traces_dict = data_utils.map_to_deeptte(test_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.json\n",
    "summary_dict = {\n",
    "    'dist_gap_mean': np.mean(train_traces['dist_calc_km']),\n",
    "    'dist_gap_std': np.std(train_traces['dist_calc_km']),\n",
    "    'time_gap_mean': np.mean(train_traces['time_calc_s']),\n",
    "    'time_gap_std': np.std(train_traces['time_calc_s']),\n",
    "    'lngs_std': np.std(train_traces['lon']),\n",
    "    'lngs_mean': np.mean(train_traces['lon']),\n",
    "    'lats_mean': np.mean(train_traces['lat']),\n",
    "    'dist_std': np.std(train_traces.groupby(['file','trip_id']).max()[['dist_cumulative_km']].values.flatten()),\n",
    "    \"dist_mean\": np.mean(train_traces.groupby(['file','trip_id']).max()[['dist_cumulative_km']].values.flatten()),\n",
    "    \"lats_std\": np.std(train_traces['lat']),\n",
    "    \"time_mean\": np.mean(train_traces.groupby(['file','trip_id']).max()[['time_cumulative_s']].values.flatten()),\n",
    "    \"time_std\": np.std(train_traces.groupby(['file','trip_id']).max()[['time_cumulative_s']].values.flatten()),\n",
    "    \"train_set\": [\"train_00\", \"train_01\", \"train_02\", \"train_03\"],\n",
    "    \"eval_set\": [\"train_04\"],\n",
    "    \"test_set\": [\"test\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save data before copying it to deeptte folder\n",
    "json_path = \"../data/deeptte_formatted/\"\n",
    "\n",
    "# Split data evenly into train/test files (must rename one to test)\n",
    "for j, obj in enumerate(list(train_traces_dict.keys())):\n",
    "    i = j % num_files\n",
    "    with open(json_path+\"train_0\"+str(i), mode='a') as out_file:\n",
    "        json.dump(train_traces_dict[obj], out_file)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "# Save separate date for test file\n",
    "for j, obj in enumerate(list(test_traces_dict.keys())):\n",
    "    with open(json_path+\"test\", mode='a') as out_file:\n",
    "        json.dump(test_traces_dict[obj], out_file)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "# Write summary dict to config file\n",
    "with open(json_path+\"config.json\", mode=\"a\") as out_file:\n",
    "    json.dump(summary_dict, out_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "valle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee7dd9afb01237dfbf26e9cc0cb1d4f149c742bc84ef8f1e9c50b404e05a47f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
