{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'database.data_utils' from '/Users/zack/Desktop/valle/src/database/data_utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import json\n",
    "\n",
    "from database import data_utils\n",
    "\n",
    "import contextily as cx\n",
    "import geopandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from shapely.ops import nearest_points\n",
    "\n",
    "import importlib\n",
    "importlib.reload(data_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get traces from all data\n",
    "train_dates = data_utils.get_date_list(\"2022_11_01\", 14)\n",
    "test_dates = data_utils.get_date_list(\"2022_11_15\", 7)\n",
    "train_data = data_utils.combine_specific_folder_data(\"../data/kcm_all\", train_dates)\n",
    "test_data = data_utils.combine_specific_folder_data(\"../data/kcm_all\", test_dates)\n",
    "train_traces = data_utils.calculate_trace_df(train_data, 'file', 'tripid', 'locationtime', 'lat', 'lon', ['orientation','scheduledeviation','tripdistance','locationtime'], 'America/Los_Angeles', use_coord_dist=True)\n",
    "test_traces = data_utils.calculate_trace_df(test_data, 'file', 'tripid', 'locationtime', 'lat', 'lon', ['orientation','scheduledeviation','tripdistance','locationtime'], 'America/Los_Angeles', use_coord_dist=True)\n",
    "\n",
    "# Save trace data for results analysis\n",
    "with open(\"../results/kcm2weeks/data/train_traces.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_traces, f)\n",
    "with open(\"../results/kcm2weeks/data/test_traces.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_traces, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get traces from all data\n",
    "# train_dates = data_utils.get_date_list(\"2022_09_01\", 14)\n",
    "# test_dates = data_utils.get_date_list(\"2022_09_14\", 7)\n",
    "# train_data = data_utils.combine_specific_folder_data(\"../data/nwy_all\", train_dates)\n",
    "# test_data = data_utils.combine_specific_folder_data(\"../data/nwy_all\", test_dates)\n",
    "# train_traces = data_utils.calculate_trace_df(train_data, 'file', 'datedvehiclejourney', 'locationtime', 'lat', 'lon', ['bearing','locationtime'], 'Europe/Oslo', use_coord_dist=True)\n",
    "# test_traces = data_utils.calculate_trace_df(test_data, 'file', 'datedvehiclejourney', 'locationtime', 'lat', 'lon', ['bearing','locationtime'], 'Europe/Oslo', use_coord_dist=True)\n",
    "\n",
    "# # Save trace data for results analysis\n",
    "# with open(\"../results/nwy2weeks/data/train_traces.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(train_traces, f)\n",
    "# with open(\"../results/nwy2weeks/data/test_traces.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(test_traces, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "['2022_11_01.pkl', '2022_11_02.pkl', '2022_11_03.pkl', '2022_11_04.pkl', '2022_11_05.pkl', '2022_11_06.pkl', '2022_11_07.pkl', '2022_11_08.pkl', '2022_11_09.pkl', '2022_11_10.pkl', '2022_11_11.pkl', '2022_11_12.pkl', '2022_11_13.pkl', '2022_11_14.pkl']\n",
      "5601830\n",
      "\n",
      "Test:\n",
      "['2022_11_15.pkl', '2022_11_16.pkl', '2022_11_17.pkl', '2022_11_18.pkl', '2022_11_19.pkl', '2022_11_20.pkl', '2022_11_21.pkl']\n",
      "3987209\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\")\n",
    "print(train_dates)\n",
    "print(len(train_traces))\n",
    "print()\n",
    "print(\"Test:\")\n",
    "print(test_dates)\n",
    "print(len(test_traces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vehicle_ids = pd.concat([train_traces['vehicleid'], test_traces['vehicleid']]).values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_vehicle_ids = pd.concat([train_traces['vehicle'], test_traces['vehicle']]).values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode vehicle ids to start from 0\n",
    "mapping = {v:k for k,v in enumerate(set(all_vehicle_ids))}\n",
    "train_recode = [mapping[y] for y in train_traces['vehicleid'].values.flatten()]\n",
    "test_recode = [mapping[y] for y in test_traces['vehicleid'].values.flatten()]\n",
    "\n",
    "train_traces['vehicleid_recode'] = train_recode\n",
    "test_traces['vehicleid_recode'] = test_recode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recode vehicle ids to start from 0\n",
    "# mapping = {v:k for k,v in enumerate(set(all_vehicle_ids))}\n",
    "# train_recode = [mapping[y] for y in train_traces['vehicle'].values.flatten()]\n",
    "# test_recode = [mapping[y] for y in test_traces['vehicle'].values.flatten()]\n",
    "\n",
    "# train_traces['vehicleid_recode'] = train_recode\n",
    "# test_traces['vehicleid_recode'] = test_recode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1148"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This must be put into models/base/Attr.py\n",
    "# It is possible that not all vehicle ids are in the training data\n",
    "len(pd.unique(all_vehicle_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_files = 5\n",
    "# If there is a minibatch of 1, it will crash DeepTTE. So we just drop a sample if true.\n",
    "if len(test_traces) % batch_size == 1:\n",
    "    test_traces = test_traces.iloc[0:len(test_traces)-1,:]\n",
    "\n",
    "# Just drop last minibatch for train data that is split across files\n",
    "extras = len(train_traces) % (batch_size * num_files)\n",
    "train_traces = train_traces.iloc[0:len(train_traces)-extras,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_traces_dict = data_utils.map_to_deeptte(train_traces, 'file', 'tripid')\n",
    "test_traces_dict = data_utils.map_to_deeptte(test_traces, 'file', 'tripid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_traces_dict = data_utils.map_to_deeptte(train_traces, 'file', 'datedvehiclejourney')\n",
    "# test_traces_dict = data_utils.map_to_deeptte(test_traces, 'file', 'datedvehiclejourney')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.json\n",
    "summary_dict = {\n",
    "    'dist_gap_mean': np.mean(train_traces['dist_calc_km']),\n",
    "    'dist_gap_std': np.std(train_traces['dist_calc_km']),\n",
    "    'time_gap_mean': np.mean(train_traces['locationtime_diff']),\n",
    "    'time_gap_std': np.std(train_traces['locationtime_diff']),\n",
    "    'lngs_std': np.std(train_traces['lon']),\n",
    "    'lngs_mean': np.mean(train_traces['lon']),\n",
    "    'lats_mean': np.mean(train_traces['lat']),\n",
    "    'dist_std': np.std(train_traces.groupby(['file','tripid']).max()[['dist_cumulative']].values.flatten()),\n",
    "    \"dist_mean\": np.mean(train_traces.groupby(['file','tripid']).max()[['dist_cumulative']].values.flatten()),\n",
    "    \"lats_std\": np.std(train_traces['lat']),\n",
    "    \"time_mean\": np.mean(train_traces.groupby(['file','tripid']).max()[['time_cumulative']].values.flatten()),\n",
    "    \"time_std\": np.std(train_traces.groupby(['file','tripid']).max()[['time_cumulative']].values.flatten()),\n",
    "    \"train_set\": [\"train_00\", \"train_01\", \"train_02\", \"train_03\"],\n",
    "    \"eval_set\": [\"train_04\"],\n",
    "    \"test_set\": [\"test\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_traces['lat'] = train_traces['lat'].astype(float)\n",
    "# train_traces['lon'] = train_traces['lon'].astype(float)\n",
    "\n",
    "# # config.json\n",
    "# summary_dict = {\n",
    "#     'dist_gap_mean': np.mean(train_traces['dist_calc_km']),\n",
    "#     'dist_gap_std': np.std(train_traces['dist_calc_km']),\n",
    "#     'time_gap_mean': np.mean(train_traces['locationtime_diff']),\n",
    "#     'time_gap_std': np.std(train_traces['locationtime_diff']),\n",
    "#     'lngs_std': np.std(train_traces['lon']),\n",
    "#     'lngs_mean': np.mean(train_traces['lon']),\n",
    "#     'lats_mean': np.mean(train_traces['lat']),\n",
    "#     'dist_std': np.std(train_traces.groupby(['file','datedvehiclejourney']).max()[['dist_cumulative']].values.flatten()),\n",
    "#     \"dist_mean\": np.mean(train_traces.groupby(['file','datedvehiclejourney']).max()[['dist_cumulative']].values.flatten()),\n",
    "#     \"lats_std\": np.std(train_traces['lat']),\n",
    "#     \"time_mean\": np.mean(train_traces.groupby(['file','datedvehiclejourney']).max()[['time_cumulative']].values.flatten()),\n",
    "#     \"time_std\": np.std(train_traces.groupby(['file','datedvehiclejourney']).max()[['time_cumulative']].values.flatten()),\n",
    "#     \"train_set\": [\"train_00\", \"train_01\", \"train_02\", \"train_03\"],\n",
    "#     \"eval_set\": [\"train_04\"],\n",
    "#     \"test_set\": [\"test\"]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save data before copying it to deeptte folder\n",
    "json_path = \"../data/deeptte_formatted/\"\n",
    "\n",
    "# Split data evenly into train/test files (must rename one to test)\n",
    "for j, obj in enumerate(list(train_traces_dict.keys())):\n",
    "    i = j % num_files\n",
    "    with open(json_path+\"train_0\"+str(i), mode='a') as out_file:\n",
    "        json.dump(train_traces_dict[obj], out_file)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "# Save separate date for test file\n",
    "for j, obj in enumerate(list(test_traces_dict.keys())):\n",
    "    with open(json_path+\"test\", mode='a') as out_file:\n",
    "        json.dump(test_traces_dict[obj], out_file)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "# Write summary dict to config file\n",
    "with open(json_path+\"config.json\", mode=\"a\") as out_file:\n",
    "    json.dump(summary_dict, out_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "valle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee7dd9afb01237dfbf26e9cc0cb1d4f149c742bc84ef8f1e9c50b404e05a47f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
