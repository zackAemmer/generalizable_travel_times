{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from database import data_utils\n",
    "from database import shape_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import importlib\n",
    "importlib.reload(data_utils)\n",
    "importlib.reload(shape_utils)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "RUN_FOLDER = \"../results/end_to_end/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lost dates train: []\n",
      "Lost dates test: []\n",
      "1147 Unique vehicle IDs (ADD TO ATTR.PY)\n"
     ]
    }
   ],
   "source": [
    "# Get traces from all data (KCM)\n",
    "given_names = ['tripid','file','locationtime','lat','lon','vehicleid']\n",
    "train_data, train_fail_dates = data_utils.combine_pkl_data(\"../data/kcm_all\", data_utils.get_date_list(\"2022_11_01\", 14), given_names)\n",
    "test_data, test_fail_dates = data_utils.combine_pkl_data(\"../data/kcm_all\", data_utils.get_date_list(\"2022_11_15\", 7), given_names)\n",
    "print(f\"Lost dates train: {train_fail_dates}\")\n",
    "print(f\"Lost dates test: {test_fail_dates}\")\n",
    "\n",
    "# Load the GTFS\n",
    "kcm_gtfs = data_utils.merge_gtfs_files(\"../data/kcm_gtfs/2022_09_19/\")\n",
    "\n",
    "# Calculate distance between points in each trajectory, do some filtering on speed, n_points\n",
    "train_traces = data_utils.calculate_trace_df(train_data, 'America/Los_Angeles')\n",
    "test_traces = data_utils.calculate_trace_df(test_data, 'America/Los_Angeles')\n",
    "\n",
    "# Match trajectories to timetables and do filtering on stop distance, availability\n",
    "train_traces = data_utils.clean_trace_df_w_timetables(train_traces, kcm_gtfs)\n",
    "test_traces = data_utils.clean_trace_df_w_timetables(test_traces, kcm_gtfs)\n",
    "\n",
    "# Get unique vehicle ids\n",
    "(train_traces, test_traces), n_unique_veh = data_utils.remap_vehicle_ids([train_traces, test_traces])\n",
    "print(f\"{n_unique_veh} Unique vehicle IDs (ADD TO ATTR.PY)\")\n",
    "train_traces.head()\n",
    "\n",
    "# Save trace data for results analysis\n",
    "with open(RUN_FOLDER+\"kcm/train_traces.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_traces, f)\n",
    "with open(RUN_FOLDER+\"kcm/test_traces.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_traces, f)\n",
    "\n",
    "# Get the trace data in format for DeepTTE (no more transformations or filters)\n",
    "train_traces_dict = data_utils.map_to_deeptte(train_traces)\n",
    "test_traces_dict = data_utils.map_to_deeptte(test_traces)\n",
    "summary_config = data_utils.get_summary_config(train_traces)\n",
    "\n",
    "# Where to save data before copying it to deeptte folder\n",
    "json_path = RUN_FOLDER+\"kcm/deeptte_formatted/\"\n",
    "num_files = 5\n",
    "\n",
    "# Delete existing files\n",
    "for file in os.listdir(json_path):\n",
    "    os.remove(json_path+file)\n",
    "\n",
    "# Split data evenly into train/test files (must rename one to test)\n",
    "for j, obj in enumerate(list(train_traces_dict.keys())):\n",
    "    i = j % num_files\n",
    "    with open(json_path+\"train_0\"+str(i), mode='a') as out_file:\n",
    "        json.dump(train_traces_dict[obj], out_file)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "# Save separate dates for test file\n",
    "for j, obj in enumerate(list(test_traces_dict.keys())):\n",
    "    with open(json_path+\"test\", mode='a') as out_file:\n",
    "        json.dump(test_traces_dict[obj], out_file)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "# Write summary dict to config file\n",
    "with open(json_path+\"config.json\", mode=\"a\") as out_file:\n",
    "    json.dump(summary_config, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lost dates train: []\n",
      "Lost dates test: []\n",
      "207 Unique vehicle IDs (ADD TO ATTR.PY)\n"
     ]
    }
   ],
   "source": [
    "# Get traces from all data (ATB)\n",
    "# For now, we can use Norway dates that are post-2022_11_02\n",
    "# Need to get mapping of old IDs to new IDs in order to use schedule data from prior to that date\n",
    "# These dates are also somewhat low on data compared to previous\n",
    "train_dates = [\n",
    "    \"2022_11_02.pkl\",\n",
    "    \"2022_11_03.pkl\",\n",
    "    \"2022_11_06.pkl\",\n",
    "    \"2022_11_07.pkl\",\n",
    "    \"2022_11_08.pkl\",\n",
    "    \"2022_11_09.pkl\",\n",
    "    \"2022_11_10.pkl\",\n",
    "    \"2022_11_13.pkl\",\n",
    "    \"2022_11_14.pkl\",\n",
    "    \"2022_11_15.pkl\",\n",
    "    \"2022_11_16.pkl\",\n",
    "    \"2022_11_17.pkl\",\n",
    "    \"2022_11_20.pkl\",\n",
    "    \"2022_11_21.pkl\"\n",
    "]\n",
    "test_dates = [\n",
    "    \"2022_11_22.pkl\",\n",
    "    \"2022_11_23.pkl\",\n",
    "    \"2022_11_24.pkl\",\n",
    "    \"2022_11_25.pkl\",\n",
    "    \"2022_11_26.pkl\",\n",
    "    \"2022_11_27.pkl\",\n",
    "    \"2022_11_28.pkl\"\n",
    "]\n",
    "given_names = ['datedvehiclejourney','file','locationtime','lat','lon','vehicle']\n",
    "train_data, train_fail_dates = data_utils.combine_pkl_data(\"../data/atb_all\", train_dates, given_names)\n",
    "test_data, test_fail_dates = data_utils.combine_pkl_data(\"../data/atb_all\", test_dates, given_names)\n",
    "print(f\"Lost dates train: {train_fail_dates}\")\n",
    "print(f\"Lost dates test: {test_fail_dates}\")\n",
    "\n",
    "# Load the GTFS\n",
    "nwy_gtfs = data_utils.merge_gtfs_files('../data/nwy_gtfs/2022_12_01/')\n",
    "\n",
    "# Calculate distance between points in each trajectory, do some filtering on speed, remap veh_id\n",
    "train_traces = data_utils.calculate_trace_df(train_data, 'Europe/Oslo')\n",
    "test_traces = data_utils.calculate_trace_df(test_data, 'Europe/Oslo')\n",
    "\n",
    "# Match trajectories to timetables and do filtering on stop distance, availability\n",
    "train_traces = data_utils.clean_trace_df_w_timetables(train_traces, nwy_gtfs)\n",
    "test_traces = data_utils.clean_trace_df_w_timetables(test_traces, nwy_gtfs)\n",
    "\n",
    "# Get unique vehicle ids\n",
    "(train_traces, test_traces), n_unique_veh = data_utils.remap_vehicle_ids([train_traces, test_traces])\n",
    "print(f\"{n_unique_veh} Unique vehicle IDs (ADD TO ATTR.PY)\")\n",
    "\n",
    "# Save trace data for results analysis\n",
    "with open(RUN_FOLDER+\"atb/train_traces.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_traces, f)\n",
    "with open(RUN_FOLDER+\"atb/test_traces.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_traces, f)\n",
    "\n",
    "# Get the trace data in format for DeepTTE (no more transformations or filters)\n",
    "train_traces_dict = data_utils.map_to_deeptte(train_traces)\n",
    "test_traces_dict = data_utils.map_to_deeptte(test_traces)\n",
    "summary_config = data_utils.get_summary_config(train_traces)\n",
    "\n",
    "# Where to save data before copying it to deeptte folder\n",
    "json_path = RUN_FOLDER+\"atb/deeptte_formatted/\"\n",
    "num_files = 5\n",
    "\n",
    "# Delete existing files\n",
    "for file in os.listdir(json_path):\n",
    "    os.remove(json_path+file)\n",
    "\n",
    "# Split data evenly into train/test files (must rename one to test)\n",
    "for j, obj in enumerate(list(train_traces_dict.keys())):\n",
    "    i = j % num_files\n",
    "    with open(json_path+\"train_0\"+str(i), mode='a') as out_file:\n",
    "        json.dump(train_traces_dict[obj], out_file)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "# Save separate dates for test file\n",
    "for j, obj in enumerate(list(test_traces_dict.keys())):\n",
    "    with open(json_path+\"test\", mode='a') as out_file:\n",
    "        json.dump(test_traces_dict[obj], out_file)\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "# Write summary dict to config file\n",
    "with open(json_path+\"config.json\", mode=\"a\") as out_file:\n",
    "    json.dump(summary_config, out_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "valle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6e860922b4cd137ba0ae084793a92306c10a3f750737db0ce768f337618f216"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
