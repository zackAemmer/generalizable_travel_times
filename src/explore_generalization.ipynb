{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import importlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "\n",
    "from models import avg_speed, schedule, ff, persistent, rnn, transformer, conv\n",
    "from utils import data_utils, data_loader, model_utils, shape_utils\n",
    "\n",
    "importlib.reload(data_utils)\n",
    "importlib.reload(data_loader)\n",
    "importlib.reload(shape_utils)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "run_folder = \"../results/debug/\"\n",
    "FOLD_MODEL = 0\n",
    "device = torch.device(\"cpu\")\n",
    "NUM_WORKERS = 4\n",
    "HIDDEN_SIZE = 32\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Set hyperparameters\n",
    "EPOCH_EVAL_FREQ = 2\n",
    "\n",
    "# Define embedded variables for network models\n",
    "embed_dict = {\n",
    "    'timeID': {\n",
    "        'vocab_size': 1440,\n",
    "        'embed_dims': 24\n",
    "    },\n",
    "    'weekID': {\n",
    "        'vocab_size': 7,\n",
    "        'embed_dims': 4\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get list of available train/test files\n",
    "kcm_data_folder = f\"{run_folder}kcm/deeptte_formatted/\"\n",
    "atb_data_folder = f\"{run_folder}atb/deeptte_formatted/\"\n",
    "\n",
    "train_file_list_kcm = list(filter(lambda x: x[:5]==\"train\" and len(x)==6, os.listdir(kcm_data_folder)))\n",
    "test_file_list_kcm = list(filter(lambda x: x[:4]==\"test\" and len(x)==5, os.listdir(kcm_data_folder)))\n",
    "train_file_list_atb = list(filter(lambda x: x[:5]==\"train\" and len(x)==6, os.listdir(atb_data_folder)))\n",
    "test_file_list_atb = list(filter(lambda x: x[:4]==\"test\" and len(x)==5, os.listdir(atb_data_folder)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load KCM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare baseline models\n",
    "avg_model = data_utils.load_pkl(f\"{run_folder}kcm/models/AVG_{FOLD_MODEL}.pkl\")\n",
    "sch_model = data_utils.load_pkl(f\"{run_folder}kcm/models/SCH_{FOLD_MODEL}.pkl\")\n",
    "tim_model = data_utils.load_pkl(f\"{run_folder}kcm/models/PER_TIM_{FOLD_MODEL}.pkl\")\n",
    "\n",
    "# Declare network models\n",
    "ff_model = ff.FF(\n",
    "    \"FF\",\n",
    "    11,\n",
    "    HIDDEN_SIZE,\n",
    "    BATCH_SIZE,\n",
    "    embed_dict,\n",
    "    device\n",
    ").to(device)\n",
    "ff_model.load_state_dict(torch.load(f\"{run_folder}kcm/models/{ff_model.model_name}_{FOLD_MODEL}.pt\"))\n",
    "print(ff_model.model_name)\n",
    "print(sum(p.numel() for p in ff_model.parameters() if p.requires_grad))\n",
    "gru_model = rnn.GRU_RNN(\n",
    "    \"GRU_RNN\",\n",
    "    8,\n",
    "    1,\n",
    "    HIDDEN_SIZE,\n",
    "    BATCH_SIZE,\n",
    "    embed_dict,\n",
    "    device\n",
    ").to(device)\n",
    "gru_model.load_state_dict(torch.load(f\"{run_folder}kcm/models/{gru_model.model_name}_{FOLD_MODEL}.pt\"))\n",
    "print(gru_model.model_name)\n",
    "print(sum(p.numel() for p in gru_model.parameters() if p.requires_grad))\n",
    "gru_mto_model = rnn.GRU_RNN_MTO(\n",
    "    \"GRU_RNN_MTO\",\n",
    "    8,\n",
    "    1,\n",
    "    HIDDEN_SIZE,\n",
    "    BATCH_SIZE,\n",
    "    embed_dict,\n",
    "    device\n",
    ").to(device)\n",
    "gru_mto_model.load_state_dict(torch.load(f\"{run_folder}kcm/models/{gru_mto_model.model_name}_{FOLD_MODEL}.pt\"))\n",
    "print(gru_mto_model.model_name)\n",
    "print(sum(p.numel() for p in gru_mto_model.parameters() if p.requires_grad))\n",
    "conv1d_model = conv.CONV(\n",
    "    \"CONV1D\",\n",
    "    8,\n",
    "    1,\n",
    "    HIDDEN_SIZE,\n",
    "    BATCH_SIZE,\n",
    "    embed_dict,\n",
    "    device\n",
    ").to(device)\n",
    "conv1d_model.load_state_dict(torch.load(f\"{run_folder}kcm/models/{conv1d_model.model_name}_{FOLD_MODEL}.pt\"))\n",
    "print(conv1d_model.model_name)\n",
    "print(sum(p.numel() for p in conv1d_model.parameters() if p.requires_grad))\n",
    "trs_model = transformer.TRANSFORMER(\n",
    "    \"TRSF_ENC\",\n",
    "    8,\n",
    "    1,\n",
    "    HIDDEN_SIZE,\n",
    "    BATCH_SIZE,\n",
    "    embed_dict,\n",
    "    device\n",
    ").to(device)\n",
    "trs_model.load_state_dict(torch.load(f\"{run_folder}kcm/models/{trs_model.model_name}_{FOLD_MODEL}.pt\"))\n",
    "print(trs_model.model_name)\n",
    "print(sum(p.numel() for p in trs_model.parameters() if p.requires_grad))\n",
    "\n",
    "# Add all models to results list\n",
    "model_list = []\n",
    "model_list.append(avg_model)\n",
    "model_list.append(sch_model)\n",
    "model_list.append(tim_model)\n",
    "model_list.append(ff_model)\n",
    "model_list.append(gru_model)\n",
    "model_list.append(gru_mto_model)\n",
    "model_list.append(conv1d_model)\n",
    "model_list.append(trs_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Evaluating KCM on KCM\")\n",
    "model_fold_results = {}\n",
    "for x in model_list:\n",
    "    model_fold_results[x.model_name] = {\"Labels\":[], \"Preds\":[]}\n",
    "for valid_file in test_file_list_kcm:\n",
    "    print(f\"VALIDATE ON FILE: {valid_file}\")\n",
    "\n",
    "    # These are fold holdouts, separate validation files are used for generalization\n",
    "    valid_data = data_utils.load_all_data(kcm_data_folder, valid_file)\n",
    "    with open(f\"{kcm_data_folder}{valid_file}_config.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # Construct dataloaders for network models\n",
    "    valid_dataloader_basic = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_collate, NUM_WORKERS)\n",
    "    valid_dataloader_seq = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_collate, NUM_WORKERS)\n",
    "    valid_dataloader_seq_mto = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_mto_collate, NUM_WORKERS)\n",
    "    valid_dataloader_trs = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.transformer_collate, NUM_WORKERS)\n",
    "    print(f\"Successfully loaded {len(valid_data)} testing samples.\")\n",
    "\n",
    "    print(f\"Evaluating {avg_model.model_name}\")\n",
    "    avg_labels, avg_preds = avg_model.predict(valid_dataloader_basic, config)\n",
    "    model_fold_results[avg_model.model_name][\"Labels\"].extend(list(avg_labels))\n",
    "    model_fold_results[avg_model.model_name][\"Preds\"].extend(list(avg_preds))\n",
    "\n",
    "    print(f\"Evaluating {sch_model.model_name}\")\n",
    "    sch_labels, sch_preds = sch_model.predict(valid_dataloader_basic, config)\n",
    "    model_fold_results[sch_model.model_name][\"Labels\"].extend(list(sch_labels))\n",
    "    model_fold_results[sch_model.model_name][\"Preds\"].extend(list(sch_preds))\n",
    "\n",
    "    print(f\"Evaluating {tim_model.model_name}\")\n",
    "    tim_labels, tim_preds = tim_model.predict(valid_dataloader_seq, config)\n",
    "    model_fold_results[tim_model.model_name][\"Labels\"].extend(list(tim_labels))\n",
    "    model_fold_results[tim_model.model_name][\"Preds\"].extend(list(tim_preds))\n",
    "\n",
    "    print(f\"Evaluating {ff_model.model_name}\")\n",
    "    ff_labels, ff_preds = ff_model.evaluate(valid_dataloader_basic, config)\n",
    "    model_fold_results[ff_model.model_name][\"Labels\"].extend(list(ff_labels))\n",
    "    model_fold_results[ff_model.model_name][\"Preds\"].extend(list(ff_preds))\n",
    "\n",
    "    print(f\"Evaluating {gru_model.model_name}\")\n",
    "    gru_labels, gru_preds = gru_model.evaluate(valid_dataloader_seq, config)\n",
    "    model_fold_results[gru_model.model_name][\"Labels\"].extend(list(gru_labels))\n",
    "    model_fold_results[gru_model.model_name][\"Preds\"].extend(list(gru_preds))\n",
    "\n",
    "    print(f\"Evaluating {gru_mto_model.model_name}\")\n",
    "    gru_mto_labels, gru_mto_preds = gru_mto_model.evaluate(valid_dataloader_seq_mto, config)\n",
    "    model_fold_results[gru_mto_model.model_name][\"Labels\"].extend(list(gru_mto_labels))\n",
    "    model_fold_results[gru_mto_model.model_name][\"Preds\"].extend(list(gru_mto_preds))\n",
    "\n",
    "    print(f\"Evaluating {conv1d_model.model_name}\")\n",
    "    conv1d_labels, conv1d_preds = conv1d_model.evaluate(valid_dataloader_seq, config)\n",
    "    model_fold_results[conv1d_model.model_name][\"Labels\"].extend(list(conv1d_labels))\n",
    "    model_fold_results[conv1d_model.model_name][\"Preds\"].extend(list(conv1d_preds))\n",
    "\n",
    "    print(f\"Evaluating {trs_model.model_name}\")\n",
    "    trs_labels, trs_preds = trs_model.evaluate(valid_dataloader_trs, config)\n",
    "    model_fold_results[trs_model.model_name][\"Labels\"].extend(list(trs_labels))\n",
    "    model_fold_results[trs_model.model_name][\"Preds\"].extend(list(trs_preds))\n",
    "kcm_kcm_model_fold_results = model_fold_results\n",
    "\n",
    "print(f\"Evaluating KCM on ATB\")\n",
    "model_fold_results = {}\n",
    "for x in model_list:\n",
    "    model_fold_results[x.model_name] = {\"Labels\":[], \"Preds\":[]}\n",
    "for valid_file in test_file_list_atb:\n",
    "    print(f\"VALIDATE ON FILE: {valid_file}\")\n",
    "\n",
    "    # These are fold holdouts, separate validation files are used for generalization\n",
    "    valid_data = data_utils.load_all_data(atb_data_folder, valid_file)\n",
    "    with open(f\"{kcm_data_folder}{valid_file}_config.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # Construct dataloaders for network models\n",
    "    valid_dataloader_basic = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_collate, NUM_WORKERS)\n",
    "    valid_dataloader_seq = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_collate, NUM_WORKERS)\n",
    "    valid_dataloader_seq_mto = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_mto_collate, NUM_WORKERS)\n",
    "    valid_dataloader_trs = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.transformer_collate, NUM_WORKERS)\n",
    "    print(f\"Successfully loaded {len(valid_data)} testing samples.\")\n",
    "\n",
    "    print(f\"Evaluating {avg_model.model_name}\")\n",
    "    avg_labels, avg_preds = avg_model.predict(valid_dataloader_basic, config)\n",
    "    model_fold_results[avg_model.model_name][\"Labels\"].extend(list(avg_labels))\n",
    "    model_fold_results[avg_model.model_name][\"Preds\"].extend(list(avg_preds))\n",
    "\n",
    "    print(f\"Evaluating {sch_model.model_name}\")\n",
    "    sch_labels, sch_preds = sch_model.predict(valid_dataloader_basic, config)\n",
    "    model_fold_results[sch_model.model_name][\"Labels\"].extend(list(sch_labels))\n",
    "    model_fold_results[sch_model.model_name][\"Preds\"].extend(list(sch_preds))\n",
    "\n",
    "    print(f\"Evaluating {tim_model.model_name}\")\n",
    "    tim_labels, tim_preds = tim_model.predict(valid_dataloader_seq, config)\n",
    "    model_fold_results[tim_model.model_name][\"Labels\"].extend(list(tim_labels))\n",
    "    model_fold_results[tim_model.model_name][\"Preds\"].extend(list(tim_preds))\n",
    "\n",
    "    print(f\"Evaluating {ff_model.model_name}\")\n",
    "    ff_labels, ff_preds = ff_model.evaluate(valid_dataloader_basic, config)\n",
    "    model_fold_results[ff_model.model_name][\"Labels\"].extend(list(ff_labels))\n",
    "    model_fold_results[ff_model.model_name][\"Preds\"].extend(list(ff_preds))\n",
    "\n",
    "    print(f\"Evaluating {gru_model.model_name}\")\n",
    "    gru_labels, gru_preds = gru_model.evaluate(valid_dataloader_seq, config)\n",
    "    model_fold_results[gru_model.model_name][\"Labels\"].extend(list(gru_labels))\n",
    "    model_fold_results[gru_model.model_name][\"Preds\"].extend(list(gru_preds))\n",
    "\n",
    "    print(f\"Evaluating {gru_mto_model.model_name}\")\n",
    "    gru_mto_labels, gru_mto_preds = gru_mto_model.evaluate(valid_dataloader_seq_mto, config)\n",
    "    model_fold_results[gru_mto_model.model_name][\"Labels\"].extend(list(gru_mto_labels))\n",
    "    model_fold_results[gru_mto_model.model_name][\"Preds\"].extend(list(gru_mto_preds))\n",
    "\n",
    "    print(f\"Evaluating {conv1d_model.model_name}\")\n",
    "    conv1d_labels, conv1d_preds = conv1d_model.evaluate(valid_dataloader_seq, config)\n",
    "    model_fold_results[conv1d_model.model_name][\"Labels\"].extend(list(conv1d_labels))\n",
    "    model_fold_results[conv1d_model.model_name][\"Preds\"].extend(list(conv1d_preds))\n",
    "\n",
    "    print(f\"Evaluating {trs_model.model_name}\")\n",
    "    trs_labels, trs_preds = trs_model.evaluate(valid_dataloader_trs, config)\n",
    "    model_fold_results[trs_model.model_name][\"Labels\"].extend(list(trs_labels))\n",
    "    model_fold_results[trs_model.model_name][\"Preds\"].extend(list(trs_preds))\n",
    "kcm_atb_model_fold_results = model_fold_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ATB Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare baseline models\n",
    "avg_model = data_utils.load_pkl(f\"{run_folder}atb/models/AVG_{FOLD_MODEL}.pkl\")\n",
    "sch_model = data_utils.load_pkl(f\"{run_folder}atb/models/SCH_{FOLD_MODEL}.pkl\")\n",
    "tim_model = data_utils.load_pkl(f\"{run_folder}atb/models/PER_TIM_{FOLD_MODEL}.pkl\")\n",
    "\n",
    "# Declare network models\n",
    "ff_model = ff.FF(\n",
    "    \"FF\",\n",
    "    11,\n",
    "    HIDDEN_SIZE,\n",
    "    BATCH_SIZE,\n",
    "    embed_dict,\n",
    "    device\n",
    ").to(device)\n",
    "ff_model.load_state_dict(torch.load(f\"{run_folder}atb/models/{ff_model.model_name}_{FOLD_MODEL}.pt\"))\n",
    "print(ff_model.model_name)\n",
    "print(sum(p.numel() for p in ff_model.parameters() if p.requires_grad))\n",
    "gru_model = rnn.GRU_RNN(\n",
    "    \"GRU_RNN\",\n",
    "    8,\n",
    "    1,\n",
    "    HIDDEN_SIZE,\n",
    "    BATCH_SIZE,\n",
    "    embed_dict,\n",
    "    device\n",
    ").to(device)\n",
    "gru_model.load_state_dict(torch.load(f\"{run_folder}atb/models/{gru_model.model_name}_{FOLD_MODEL}.pt\"))\n",
    "print(gru_model.model_name)\n",
    "print(sum(p.numel() for p in gru_model.parameters() if p.requires_grad))\n",
    "gru_mto_model = rnn.GRU_RNN_MTO(\n",
    "    \"GRU_RNN_MTO\",\n",
    "    8,\n",
    "    1,\n",
    "    HIDDEN_SIZE,\n",
    "    BATCH_SIZE,\n",
    "    embed_dict,\n",
    "    device\n",
    ").to(device)\n",
    "gru_mto_model.load_state_dict(torch.load(f\"{run_folder}atb/models/{gru_mto_model.model_name}_{FOLD_MODEL}.pt\"))\n",
    "print(gru_mto_model.model_name)\n",
    "print(sum(p.numel() for p in gru_mto_model.parameters() if p.requires_grad))\n",
    "conv1d_model = conv.CONV(\n",
    "    \"CONV1D\",\n",
    "    8,\n",
    "    1,\n",
    "    HIDDEN_SIZE,\n",
    "    BATCH_SIZE,\n",
    "    embed_dict,\n",
    "    device\n",
    ").to(device)\n",
    "conv1d_model.load_state_dict(torch.load(f\"{run_folder}atb/models/{conv1d_model.model_name}_{FOLD_MODEL}.pt\"))\n",
    "print(conv1d_model.model_name)\n",
    "print(sum(p.numel() for p in conv1d_model.parameters() if p.requires_grad))\n",
    "trs_model = transformer.TRANSFORMER(\n",
    "    \"TRSF_ENC\",\n",
    "    8,\n",
    "    1,\n",
    "    HIDDEN_SIZE,\n",
    "    BATCH_SIZE,\n",
    "    embed_dict,\n",
    "    device\n",
    ").to(device)\n",
    "trs_model.load_state_dict(torch.load(f\"{run_folder}atb/models/{trs_model.model_name}_{FOLD_MODEL}.pt\"))\n",
    "print(trs_model.model_name)\n",
    "print(sum(p.numel() for p in trs_model.parameters() if p.requires_grad))\n",
    "\n",
    "# Add all models to results list\n",
    "model_list = []\n",
    "model_list.append(avg_model)\n",
    "model_list.append(sch_model)\n",
    "model_list.append(tim_model)\n",
    "model_list.append(ff_model)\n",
    "model_list.append(gru_model)\n",
    "model_list.append(gru_mto_model)\n",
    "model_list.append(conv1d_model)\n",
    "model_list.append(trs_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Evaluating ATB on ATB\")\n",
    "model_fold_results = {}\n",
    "for x in model_list:\n",
    "    model_fold_results[x.model_name] = {\"Labels\":[], \"Preds\":[]}\n",
    "for valid_file in test_file_list_atb:\n",
    "    print(f\"VALIDATE ON FILE: {valid_file}\")\n",
    "\n",
    "    # These are fold holdouts, separate validation files are used for generalization\n",
    "    valid_data = data_utils.load_all_data(atb_data_folder, valid_file)\n",
    "    with open(f\"{atb_data_folder}{valid_file}_config.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # Construct dataloaders for network models\n",
    "    valid_dataloader_basic = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_collate, NUM_WORKERS)\n",
    "    valid_dataloader_seq = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_collate, NUM_WORKERS)\n",
    "    valid_dataloader_seq_mto = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_mto_collate, NUM_WORKERS)\n",
    "    valid_dataloader_trs = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.transformer_collate, NUM_WORKERS)\n",
    "    print(f\"Successfully loaded {len(valid_data)} testing samples.\")\n",
    "\n",
    "    print(f\"Evaluating {avg_model.model_name}\")\n",
    "    avg_labels, avg_preds = avg_model.predict(valid_dataloader_basic, config)\n",
    "    model_fold_results[avg_model.model_name][\"Labels\"].extend(list(avg_labels))\n",
    "    model_fold_results[avg_model.model_name][\"Preds\"].extend(list(avg_preds))\n",
    "\n",
    "    print(f\"Evaluating {sch_model.model_name}\")\n",
    "    sch_labels, sch_preds = sch_model.predict(valid_dataloader_basic, config)\n",
    "    model_fold_results[sch_model.model_name][\"Labels\"].extend(list(sch_labels))\n",
    "    model_fold_results[sch_model.model_name][\"Preds\"].extend(list(sch_preds))\n",
    "\n",
    "    print(f\"Evaluating {tim_model.model_name}\")\n",
    "    tim_labels, tim_preds = tim_model.predict(valid_dataloader_seq, config)\n",
    "    model_fold_results[tim_model.model_name][\"Labels\"].extend(list(tim_labels))\n",
    "    model_fold_results[tim_model.model_name][\"Preds\"].extend(list(tim_preds))\n",
    "\n",
    "    print(f\"Evaluating {ff_model.model_name}\")\n",
    "    ff_labels, ff_preds = ff_model.evaluate(valid_dataloader_basic, config)\n",
    "    model_fold_results[ff_model.model_name][\"Labels\"].extend(list(ff_labels))\n",
    "    model_fold_results[ff_model.model_name][\"Preds\"].extend(list(ff_preds))\n",
    "\n",
    "    print(f\"Evaluating {gru_model.model_name}\")\n",
    "    gru_labels, gru_preds = gru_model.evaluate(valid_dataloader_seq, config)\n",
    "    model_fold_results[gru_model.model_name][\"Labels\"].extend(list(gru_labels))\n",
    "    model_fold_results[gru_model.model_name][\"Preds\"].extend(list(gru_preds))\n",
    "\n",
    "    print(f\"Evaluating {gru_mto_model.model_name}\")\n",
    "    gru_mto_labels, gru_mto_preds = gru_mto_model.evaluate(valid_dataloader_seq_mto, config)\n",
    "    model_fold_results[gru_mto_model.model_name][\"Labels\"].extend(list(gru_mto_labels))\n",
    "    model_fold_results[gru_mto_model.model_name][\"Preds\"].extend(list(gru_mto_preds))\n",
    "\n",
    "    print(f\"Evaluating {conv1d_model.model_name}\")\n",
    "    conv1d_labels, conv1d_preds = conv1d_model.evaluate(valid_dataloader_seq, config)\n",
    "    model_fold_results[conv1d_model.model_name][\"Labels\"].extend(list(conv1d_labels))\n",
    "    model_fold_results[conv1d_model.model_name][\"Preds\"].extend(list(conv1d_preds))\n",
    "\n",
    "    print(f\"Evaluating {trs_model.model_name}\")\n",
    "    trs_labels, trs_preds = trs_model.evaluate(valid_dataloader_trs, config)\n",
    "    model_fold_results[trs_model.model_name][\"Labels\"].extend(list(trs_labels))\n",
    "    model_fold_results[trs_model.model_name][\"Preds\"].extend(list(trs_preds))\n",
    "atb_atb_model_fold_results = model_fold_results\n",
    "\n",
    "print(f\"Evaluating ATB on KCM\")\n",
    "model_fold_results = {}\n",
    "for x in model_list:\n",
    "    model_fold_results[x.model_name] = {\"Labels\":[], \"Preds\":[]}\n",
    "for valid_file in test_file_list_kcm:\n",
    "    print(f\"VALIDATE ON FILE: {valid_file}\")\n",
    "\n",
    "    # These are fold holdouts, separate validation files are used for generalization\n",
    "    valid_data = data_utils.load_all_data(kcm_data_folder, valid_file)\n",
    "    with open(f\"{kcm_data_folder}{valid_file}_config.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # Construct dataloaders for network models\n",
    "    valid_dataloader_basic = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_collate, NUM_WORKERS)\n",
    "    valid_dataloader_seq = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_collate, NUM_WORKERS)\n",
    "    valid_dataloader_seq_mto = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_mto_collate, NUM_WORKERS)\n",
    "    valid_dataloader_trs = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.transformer_collate, NUM_WORKERS)\n",
    "    print(f\"Successfully loaded {len(valid_data)} testing samples.\")\n",
    "\n",
    "    print(f\"Evaluating {avg_model.model_name}\")\n",
    "    avg_labels, avg_preds = avg_model.predict(valid_dataloader_basic, config)\n",
    "    model_fold_results[avg_model.model_name][\"Labels\"].extend(list(avg_labels))\n",
    "    model_fold_results[avg_model.model_name][\"Preds\"].extend(list(avg_preds))\n",
    "\n",
    "    print(f\"Evaluating {sch_model.model_name}\")\n",
    "    sch_labels, sch_preds = sch_model.predict(valid_dataloader_basic, config)\n",
    "    model_fold_results[sch_model.model_name][\"Labels\"].extend(list(sch_labels))\n",
    "    model_fold_results[sch_model.model_name][\"Preds\"].extend(list(sch_preds))\n",
    "\n",
    "    print(f\"Evaluating {tim_model.model_name}\")\n",
    "    tim_labels, tim_preds = tim_model.predict(valid_dataloader_seq, config)\n",
    "    model_fold_results[tim_model.model_name][\"Labels\"].extend(list(tim_labels))\n",
    "    model_fold_results[tim_model.model_name][\"Preds\"].extend(list(tim_preds))\n",
    "\n",
    "    print(f\"Evaluating {ff_model.model_name}\")\n",
    "    ff_labels, ff_preds = ff_model.evaluate(valid_dataloader_basic, config)\n",
    "    model_fold_results[ff_model.model_name][\"Labels\"].extend(list(ff_labels))\n",
    "    model_fold_results[ff_model.model_name][\"Preds\"].extend(list(ff_preds))\n",
    "\n",
    "    print(f\"Evaluating {gru_model.model_name}\")\n",
    "    gru_labels, gru_preds = gru_model.evaluate(valid_dataloader_seq, config)\n",
    "    model_fold_results[gru_model.model_name][\"Labels\"].extend(list(gru_labels))\n",
    "    model_fold_results[gru_model.model_name][\"Preds\"].extend(list(gru_preds))\n",
    "\n",
    "    print(f\"Evaluating {gru_mto_model.model_name}\")\n",
    "    gru_mto_labels, gru_mto_preds = gru_mto_model.evaluate(valid_dataloader_seq_mto, config)\n",
    "    model_fold_results[gru_mto_model.model_name][\"Labels\"].extend(list(gru_mto_labels))\n",
    "    model_fold_results[gru_mto_model.model_name][\"Preds\"].extend(list(gru_mto_preds))\n",
    "\n",
    "    print(f\"Evaluating {conv1d_model.model_name}\")\n",
    "    conv1d_labels, conv1d_preds = conv1d_model.evaluate(valid_dataloader_seq, config)\n",
    "    model_fold_results[conv1d_model.model_name][\"Labels\"].extend(list(conv1d_labels))\n",
    "    model_fold_results[conv1d_model.model_name][\"Preds\"].extend(list(conv1d_preds))\n",
    "\n",
    "    print(f\"Evaluating {trs_model.model_name}\")\n",
    "    trs_labels, trs_preds = trs_model.evaluate(valid_dataloader_trs, config)\n",
    "    model_fold_results[trs_model.model_name][\"Labels\"].extend(list(trs_labels))\n",
    "    model_fold_results[trs_model.model_name][\"Preds\"].extend(list(trs_preds))\n",
    "atb_kcm_model_fold_results = model_fold_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "train_nets = []\n",
    "test_nets = []\n",
    "errors = []\n",
    "results = kcm_kcm_model_fold_results\n",
    "for model_name in results.keys():\n",
    "    models.append(model_name)\n",
    "    train_nets.append(\"KCM\")\n",
    "    test_nets.append(\"KCM\")\n",
    "    errors.append(np.round(np.sqrt(metrics.mean_squared_error(results[model_name]['Labels'], results[model_name]['Preds'])), 2))\n",
    "results = kcm_atb_model_fold_results\n",
    "for model_name in results.keys():\n",
    "    models.append(model_name)\n",
    "    train_nets.append(\"KCM\")\n",
    "    test_nets.append(\"ATB\")\n",
    "    errors.append(np.round(np.sqrt(metrics.mean_squared_error(results[model_name]['Labels'], results[model_name]['Preds'])), 2))\n",
    "results = atb_atb_model_fold_results\n",
    "for model_name in results.keys():\n",
    "    models.append(model_name)\n",
    "    train_nets.append(\"ATB\")\n",
    "    test_nets.append(\"ATB\")\n",
    "    errors.append(np.round(np.sqrt(metrics.mean_squared_error(results[model_name]['Labels'], results[model_name]['Preds'])), 2))\n",
    "results = atb_kcm_model_fold_results\n",
    "for model_name in results.keys():\n",
    "    models.append(model_name)\n",
    "    train_nets.append(\"ATB\")\n",
    "    test_nets.append(\"KCM\")\n",
    "    errors.append(np.round(np.sqrt(metrics.mean_squared_error(results[model_name]['Labels'], results[model_name]['Preds'])), 2))\n",
    "\n",
    "metric_results = pd.DataFrame(columns=[\"Model\",\"Train Network\",\"Test Network\",\"Metric\"])\n",
    "metric_results['Model'] = models\n",
    "metric_results['Train Network'] = train_nets\n",
    "metric_results['Test Network'] = test_nets\n",
    "metric_results['Metric'] = errors\n",
    "metric_results['Model-Train-Test'] = metric_results['Model']+\"_\"+metric_results['Train Network']+\"_\"+metric_results['Test Network']\n",
    "metric_results = metric_results.sort_values('Model-Train-Test')\n",
    "metric_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Include DeepTTE Generalization Results\n",
    "# kcm_deeptte_gen_res = data_utils.extract_deeptte_results(\"KCM\", run_folder, \"kcm/\", generalization_flag=True)\n",
    "# atb_deeptte_gen_res = data_utils.extract_deeptte_results(\"ATB\", run_folder, \"atb/\", generalization_flag=True)\n",
    "# deeptte_gen_res = pd.concat([kcm_deeptte_gen_res, atb_deeptte_gen_res])\n",
    "# deeptte_gen_res[\"Model\"] = \"DEEPTTE\"\n",
    "# deeptte_gen_res[\"Train Network\"] = deeptte_gen_res[\"City\"]\n",
    "# deeptte_gen_res[\"Test Network\"] = deeptte_gen_res[\"Loss Set\"]\n",
    "# deeptte_gen_res[\"Metric\"] = deeptte_gen_res[\"RMSE\"]\n",
    "# deeptte_gen_res = deeptte_gen_res[[\"Model\",\"Train Network\",\"Test Network\",\"Metric\"]]\n",
    "# deeptte_gen_res['Model-Train-Test'] = deeptte_gen_res['Model']+\"_\"+deeptte_gen_res['Train Network']+\"_\"+deeptte_gen_res['Test Network']\n",
    "# metric_results = pd.concat([metric_results, deeptte_gen_res])\n",
    "# metric_results = metric_results.sort_values('Model-Train-Test')\n",
    "# metric_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = metric_results[metric_results['Test Network']==\"KCM\"]\n",
    "fig, axes = plt.subplots(1,1)\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(6)\n",
    "sns.barplot(plot_data, x=\"Metric\", y=\"Model-Train-Test\", hue=\"Model\", dodge=False)\n",
    "axes.set_ylabel(\"Model\")\n",
    "axes.set_xlabel(\"RMSE\")\n",
    "fig.suptitle('Model Generalization Performance On KCM', fontsize=16)\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"../plots/model_generalization.png\", dpi=1800, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = metric_results[metric_results['Test Network']==\"ATB\"]\n",
    "fig, axes = plt.subplots(1,1)\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(6)\n",
    "sns.barplot(plot_data, x=\"Metric\", y=\"Model-Train-Test\", hue=\"Model\", dodge=False)\n",
    "axes.set_ylabel(\"Model\")\n",
    "axes.set_xlabel(\"RMSE\")\n",
    "fig.suptitle('Model Generalization Performance On ATB', fontsize=16)\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"../plots/model_generalization.png\", dpi=1800, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "valle_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b30fe1de1713ca8e7537eef068b13a2de77ded03f86aab2e80ea73416dd3d704"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
