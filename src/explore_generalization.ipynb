{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import importlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "\n",
    "from models import avg_speed, schedule, ff, persistent, rnn, transformer, conv\n",
    "from utils import data_utils, data_loader, model_utils, shape_utils\n",
    "\n",
    "importlib.reload(data_utils)\n",
    "importlib.reload(data_loader)\n",
    "importlib.reload(shape_utils)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "run_folder = \"../results/medium/\"\n",
    "run_folder_tte = \"../results/small/\"\n",
    "FOLD_MODEL = 0\n",
    "device = torch.device(\"cpu\")\n",
    "NUM_WORKERS = 4\n",
    "HIDDEN_SIZE = 32\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Set hyperparameters\n",
    "EPOCH_EVAL_FREQ = 2\n",
    "\n",
    "# Define embedded variables for network models\n",
    "embed_dict = {\n",
    "    'timeID': {\n",
    "        'vocab_size': 1440,\n",
    "        'embed_dims': 24\n",
    "    },\n",
    "    'weekID': {\n",
    "        'vocab_size': 7,\n",
    "        'embed_dims': 4\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get list of available train/test files\n",
    "kcm_data_folder = f\"{run_folder}kcm/deeptte_formatted/\"\n",
    "atb_data_folder = f\"{run_folder}atb/deeptte_formatted/\"\n",
    "\n",
    "train_file_list_kcm = list(filter(lambda x: x[:5]==\"train\" and len(x)==6, os.listdir(kcm_data_folder)))\n",
    "test_file_list_kcm = list(filter(lambda x: x[:4]==\"test\" and len(x)==5, os.listdir(kcm_data_folder)))\n",
    "train_file_list_atb = list(filter(lambda x: x[:5]==\"train\" and len(x)==6, os.listdir(atb_data_folder)))\n",
    "test_file_list_atb = list(filter(lambda x: x[:4]==\"test\" and len(x)==5, os.listdir(atb_data_folder)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load KCM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FF\n",
      "36957\n",
      "GRU_RNN\n",
      "38681\n",
      "GRU_RNN_MTO\n",
      "38681\n",
      "CONV1D\n",
      "35449\n",
      "TRSF_ENC\n",
      "50313\n"
     ]
    }
   ],
   "source": [
    "# Declare baseline models\n",
    "avg_model = data_utils.load_pkl(f\"{run_folder}kcm/models/AVG_{FOLD_MODEL}.pkl\")\n",
    "sch_model = data_utils.load_pkl(f\"{run_folder}kcm/models/SCH_{FOLD_MODEL}.pkl\")\n",
    "tim_model = data_utils.load_pkl(f\"{run_folder}kcm/models/PER_TIM_{FOLD_MODEL}.pkl\")\n",
    "\n",
    "# Declare network models\n",
    "ff_model = ff.FF(\n",
    "    \"FF\",\n",
    "    11,\n",
    "    HIDDEN_SIZE,\n",
    "    BATCH_SIZE,\n",
    "    embed_dict,\n",
    "    device\n",
    ").to(device)\n",
    "ff_model.load_state_dict(torch.load(f\"{run_folder}kcm/models/{ff_model.model_name}_{FOLD_MODEL}.pt\"))\n",
    "print(ff_model.model_name)\n",
    "print(sum(p.numel() for p in ff_model.parameters() if p.requires_grad))\n",
    "gru_model = rnn.GRU_RNN(\n",
    "    \"GRU_RNN\",\n",
    "    8,\n",
    "    1,\n",
    "    HIDDEN_SIZE,\n",
    "    BATCH_SIZE,\n",
    "    embed_dict,\n",
    "    device\n",
    ").to(device)\n",
    "gru_model.load_state_dict(torch.load(f\"{run_folder}kcm/models/{gru_model.model_name}_{FOLD_MODEL}.pt\"))\n",
    "print(gru_model.model_name)\n",
    "print(sum(p.numel() for p in gru_model.parameters() if p.requires_grad))\n",
    "gru_mto_model = rnn.GRU_RNN_MTO(\n",
    "    \"GRU_RNN_MTO\",\n",
    "    8,\n",
    "    1,\n",
    "    HIDDEN_SIZE,\n",
    "    BATCH_SIZE,\n",
    "    embed_dict,\n",
    "    device\n",
    ").to(device)\n",
    "gru_mto_model.load_state_dict(torch.load(f\"{run_folder}kcm/models/{gru_mto_model.model_name}_{FOLD_MODEL}.pt\"))\n",
    "print(gru_mto_model.model_name)\n",
    "print(sum(p.numel() for p in gru_mto_model.parameters() if p.requires_grad))\n",
    "conv1d_model = conv.CONV(\n",
    "    \"CONV1D\",\n",
    "    8,\n",
    "    1,\n",
    "    HIDDEN_SIZE,\n",
    "    BATCH_SIZE,\n",
    "    embed_dict,\n",
    "    device\n",
    ").to(device)\n",
    "conv1d_model.load_state_dict(torch.load(f\"{run_folder}kcm/models/{conv1d_model.model_name}_{FOLD_MODEL}.pt\"))\n",
    "print(conv1d_model.model_name)\n",
    "print(sum(p.numel() for p in conv1d_model.parameters() if p.requires_grad))\n",
    "trs_model = transformer.TRANSFORMER(\n",
    "    \"TRSF_ENC\",\n",
    "    8,\n",
    "    1,\n",
    "    HIDDEN_SIZE,\n",
    "    BATCH_SIZE,\n",
    "    embed_dict,\n",
    "    device\n",
    ").to(device)\n",
    "trs_model.load_state_dict(torch.load(f\"{run_folder}kcm/models/{trs_model.model_name}_{FOLD_MODEL}.pt\"))\n",
    "print(trs_model.model_name)\n",
    "print(sum(p.numel() for p in trs_model.parameters() if p.requires_grad))\n",
    "\n",
    "# Add all models to results list\n",
    "model_list = []\n",
    "model_list.append(avg_model)\n",
    "model_list.append(sch_model)\n",
    "model_list.append(tim_model)\n",
    "model_list.append(ff_model)\n",
    "model_list.append(gru_model)\n",
    "model_list.append(gru_mto_model)\n",
    "model_list.append(conv1d_model)\n",
    "model_list.append(trs_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating KCM on KCM\n",
      "VALIDATE ON FILE: test3\n",
      "Successfully loaded 12979 testing samples.\n",
      "Evaluating AVG\n",
      "Evaluating SCH\n",
      "Evaluating PER_TIM\n",
      "Evaluating FF\n",
      "Evaluating GRU_RNN\n",
      "Evaluating GRU_RNN_MTO\n",
      "Evaluating CONV1D\n",
      "Evaluating TRSF_ENC\n",
      "VALIDATE ON FILE: test1\n",
      "Successfully loaded 40600 testing samples.\n",
      "Evaluating AVG\n",
      "Evaluating SCH\n",
      "Evaluating PER_TIM\n",
      "Evaluating FF\n",
      "Evaluating GRU_RNN\n",
      "Evaluating GRU_RNN_MTO\n",
      "Evaluating CONV1D\n",
      "Evaluating TRSF_ENC\n",
      "VALIDATE ON FILE: test0\n",
      "Successfully loaded 41912 testing samples.\n",
      "Evaluating AVG\n",
      "Evaluating SCH\n",
      "Evaluating PER_TIM\n",
      "Evaluating FF\n",
      "Evaluating GRU_RNN\n",
      "Evaluating GRU_RNN_MTO\n",
      "Evaluating CONV1D\n",
      "Evaluating TRSF_ENC\n",
      "VALIDATE ON FILE: test2\n",
      "Successfully loaded 13522 testing samples.\n",
      "Evaluating AVG\n",
      "Evaluating SCH\n",
      "Evaluating PER_TIM\n",
      "Evaluating FF\n",
      "Evaluating GRU_RNN\n",
      "Evaluating GRU_RNN_MTO\n",
      "Evaluating CONV1D\n",
      "Evaluating TRSF_ENC\n",
      "VALIDATE ON FILE: test4\n",
      "Successfully loaded 20084 testing samples.\n",
      "Evaluating AVG\n",
      "Evaluating SCH\n",
      "Evaluating PER_TIM\n",
      "Evaluating FF\n",
      "Evaluating GRU_RNN\n",
      "Evaluating GRU_RNN_MTO\n",
      "Evaluating CONV1D\n",
      "Evaluating TRSF_ENC\n",
      "Evaluating KCM on ATB\n",
      "VALIDATE ON FILE: test3\n",
      "Successfully loaded 1217 testing samples.\n",
      "Evaluating AVG\n",
      "Evaluating SCH\n",
      "Evaluating PER_TIM\n",
      "Evaluating FF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zack/miniconda3/envs/valle_m1/lib/python3.9/site-packages/torch/nn/modules/loss.py:988: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.huber_loss(input, target, reduction=self.reduction, delta=self.delta)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "zero-dimensional tensor (at position 19) cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 96\u001b[0m\n\u001b[1;32m     93\u001b[0m model_fold_results[tim_model\u001b[39m.\u001b[39mmodel_name][\u001b[39m\"\u001b[39m\u001b[39mPreds\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mextend(\u001b[39mlist\u001b[39m(tim_preds))\n\u001b[1;32m     95\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEvaluating \u001b[39m\u001b[39m{\u001b[39;00mff_model\u001b[39m.\u001b[39mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m ff_labels, ff_preds \u001b[39m=\u001b[39m ff_model\u001b[39m.\u001b[39;49mevaluate(valid_dataloader_basic, config)\n\u001b[1;32m     97\u001b[0m model_fold_results[ff_model\u001b[39m.\u001b[39mmodel_name][\u001b[39m\"\u001b[39m\u001b[39mLabels\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mextend(\u001b[39mlist\u001b[39m(ff_labels))\n\u001b[1;32m     98\u001b[0m model_fold_results[ff_model\u001b[39m.\u001b[39mmodel_name][\u001b[39m\"\u001b[39m\u001b[39mPreds\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mextend(\u001b[39mlist\u001b[39m(ff_preds))\n",
      "File \u001b[0;32m~/Skrivebord/valle/src/models/ff.py:50\u001b[0m, in \u001b[0;36mFF.evaluate\u001b[0;34m(self, test_dataloader, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate\u001b[39m(\u001b[39mself\u001b[39m, test_dataloader, config):\n\u001b[0;32m---> 50\u001b[0m     labels, preds, avg_batch_loss \u001b[39m=\u001b[39m model_utils\u001b[39m.\u001b[39;49mpredict(\u001b[39mself\u001b[39;49m, test_dataloader)\n\u001b[1;32m     51\u001b[0m     labels \u001b[39m=\u001b[39m data_utils\u001b[39m.\u001b[39mde_normalize(labels, config[\u001b[39m'\u001b[39m\u001b[39mtime_mean\u001b[39m\u001b[39m'\u001b[39m], config[\u001b[39m'\u001b[39m\u001b[39mtime_std\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     52\u001b[0m     preds \u001b[39m=\u001b[39m data_utils\u001b[39m.\u001b[39mde_normalize(preds, config[\u001b[39m'\u001b[39m\u001b[39mtime_mean\u001b[39m\u001b[39m'\u001b[39m], config[\u001b[39m'\u001b[39m\u001b[39mtime_std\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/Skrivebord/valle/src/utils/model_utils.py:58\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model, dataloader, sequential_flag)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcat(labels)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m---> 58\u001b[0m     preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mconcat(preds)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     59\u001b[0m \u001b[39mreturn\u001b[39;00m labels, preds, avg_batch_loss \u001b[39m/\u001b[39m num_batches\n",
      "\u001b[0;31mRuntimeError\u001b[0m: zero-dimensional tensor (at position 19) cannot be concatenated"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluating KCM on KCM\")\n",
    "model_fold_results = {}\n",
    "for x in model_list:\n",
    "    model_fold_results[x.model_name] = {\"Labels\":[], \"Preds\":[]}\n",
    "for valid_file in test_file_list_kcm:\n",
    "    print(f\"VALIDATE ON FILE: {valid_file}\")\n",
    "\n",
    "    # These are fold holdouts, separate validation files are used for generalization\n",
    "    valid_data = data_utils.load_all_data(kcm_data_folder, valid_file)\n",
    "    with open(f\"{kcm_data_folder}train_config.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # Construct dataloaders for network models\n",
    "    valid_dataloader_basic = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_collate, NUM_WORKERS)\n",
    "    valid_dataloader_seq = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_collate, NUM_WORKERS)\n",
    "    valid_dataloader_seq_mto = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_mto_collate, NUM_WORKERS)\n",
    "    valid_dataloader_trs = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.transformer_collate, NUM_WORKERS)\n",
    "    print(f\"Successfully loaded {len(valid_data)} testing samples.\")\n",
    "\n",
    "    print(f\"Evaluating {avg_model.model_name}\")\n",
    "    avg_labels, avg_preds = avg_model.predict(valid_dataloader_basic, config)\n",
    "    model_fold_results[avg_model.model_name][\"Labels\"].extend(list(avg_labels))\n",
    "    model_fold_results[avg_model.model_name][\"Preds\"].extend(list(avg_preds))\n",
    "\n",
    "    print(f\"Evaluating {sch_model.model_name}\")\n",
    "    sch_labels, sch_preds = sch_model.predict(valid_dataloader_basic, config)\n",
    "    model_fold_results[sch_model.model_name][\"Labels\"].extend(list(sch_labels))\n",
    "    model_fold_results[sch_model.model_name][\"Preds\"].extend(list(sch_preds))\n",
    "\n",
    "    print(f\"Evaluating {tim_model.model_name}\")\n",
    "    tim_labels, tim_preds = tim_model.predict(valid_dataloader_seq, config)\n",
    "    model_fold_results[tim_model.model_name][\"Labels\"].extend(list(tim_labels))\n",
    "    model_fold_results[tim_model.model_name][\"Preds\"].extend(list(tim_preds))\n",
    "\n",
    "    print(f\"Evaluating {ff_model.model_name}\")\n",
    "    ff_labels, ff_preds = ff_model.evaluate(valid_dataloader_basic, config)\n",
    "    model_fold_results[ff_model.model_name][\"Labels\"].extend(list(ff_labels))\n",
    "    model_fold_results[ff_model.model_name][\"Preds\"].extend(list(ff_preds))\n",
    "\n",
    "    print(f\"Evaluating {gru_model.model_name}\")\n",
    "    gru_labels, gru_preds = gru_model.evaluate(valid_dataloader_seq, config)\n",
    "    model_fold_results[gru_model.model_name][\"Labels\"].extend(list(gru_labels))\n",
    "    model_fold_results[gru_model.model_name][\"Preds\"].extend(list(gru_preds))\n",
    "\n",
    "    print(f\"Evaluating {gru_mto_model.model_name}\")\n",
    "    gru_mto_labels, gru_mto_preds = gru_mto_model.evaluate(valid_dataloader_seq_mto, config)\n",
    "    model_fold_results[gru_mto_model.model_name][\"Labels\"].extend(list(gru_mto_labels))\n",
    "    model_fold_results[gru_mto_model.model_name][\"Preds\"].extend(list(gru_mto_preds))\n",
    "\n",
    "    print(f\"Evaluating {conv1d_model.model_name}\")\n",
    "    conv1d_labels, conv1d_preds = conv1d_model.evaluate(valid_dataloader_seq, config)\n",
    "    model_fold_results[conv1d_model.model_name][\"Labels\"].extend(list(conv1d_labels))\n",
    "    model_fold_results[conv1d_model.model_name][\"Preds\"].extend(list(conv1d_preds))\n",
    "\n",
    "    print(f\"Evaluating {trs_model.model_name}\")\n",
    "    trs_labels, trs_preds = trs_model.evaluate(valid_dataloader_trs, config)\n",
    "    model_fold_results[trs_model.model_name][\"Labels\"].extend(list(trs_labels))\n",
    "    model_fold_results[trs_model.model_name][\"Preds\"].extend(list(trs_preds))\n",
    "kcm_kcm_model_fold_results = model_fold_results\n",
    "\n",
    "print(f\"Evaluating KCM on ATB\")\n",
    "model_fold_results = {}\n",
    "for x in model_list:\n",
    "    model_fold_results[x.model_name] = {\"Labels\":[], \"Preds\":[]}\n",
    "for valid_file in test_file_list_atb:\n",
    "    print(f\"VALIDATE ON FILE: {valid_file}\")\n",
    "\n",
    "    # These are fold holdouts, separate validation files are used for generalization\n",
    "    valid_data = data_utils.load_all_data(atb_data_folder, valid_file)\n",
    "    with open(f\"{kcm_data_folder}train_config.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # Construct dataloaders for network models\n",
    "    valid_dataloader_basic = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_collate, NUM_WORKERS)\n",
    "    valid_dataloader_seq = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_collate, NUM_WORKERS)\n",
    "    valid_dataloader_seq_mto = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_mto_collate, NUM_WORKERS)\n",
    "    valid_dataloader_trs = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.transformer_collate, NUM_WORKERS)\n",
    "    print(f\"Successfully loaded {len(valid_data)} testing samples.\")\n",
    "\n",
    "    print(f\"Evaluating {avg_model.model_name}\")\n",
    "    avg_labels, avg_preds = avg_model.predict(valid_dataloader_basic, config)\n",
    "    model_fold_results[avg_model.model_name][\"Labels\"].extend(list(avg_labels))\n",
    "    model_fold_results[avg_model.model_name][\"Preds\"].extend(list(avg_preds))\n",
    "\n",
    "    print(f\"Evaluating {sch_model.model_name}\")\n",
    "    sch_labels, sch_preds = sch_model.predict(valid_dataloader_basic, config)\n",
    "    model_fold_results[sch_model.model_name][\"Labels\"].extend(list(sch_labels))\n",
    "    model_fold_results[sch_model.model_name][\"Preds\"].extend(list(sch_preds))\n",
    "\n",
    "    print(f\"Evaluating {tim_model.model_name}\")\n",
    "    tim_labels, tim_preds = tim_model.predict(valid_dataloader_seq, config)\n",
    "    model_fold_results[tim_model.model_name][\"Labels\"].extend(list(tim_labels))\n",
    "    model_fold_results[tim_model.model_name][\"Preds\"].extend(list(tim_preds))\n",
    "\n",
    "    print(f\"Evaluating {ff_model.model_name}\")\n",
    "    ff_labels, ff_preds = ff_model.evaluate(valid_dataloader_basic, config)\n",
    "    model_fold_results[ff_model.model_name][\"Labels\"].extend(list(ff_labels))\n",
    "    model_fold_results[ff_model.model_name][\"Preds\"].extend(list(ff_preds))\n",
    "\n",
    "    print(f\"Evaluating {gru_model.model_name}\")\n",
    "    gru_labels, gru_preds = gru_model.evaluate(valid_dataloader_seq, config)\n",
    "    model_fold_results[gru_model.model_name][\"Labels\"].extend(list(gru_labels))\n",
    "    model_fold_results[gru_model.model_name][\"Preds\"].extend(list(gru_preds))\n",
    "\n",
    "    print(f\"Evaluating {gru_mto_model.model_name}\")\n",
    "    gru_mto_labels, gru_mto_preds = gru_mto_model.evaluate(valid_dataloader_seq_mto, config)\n",
    "    model_fold_results[gru_mto_model.model_name][\"Labels\"].extend(list(gru_mto_labels))\n",
    "    model_fold_results[gru_mto_model.model_name][\"Preds\"].extend(list(gru_mto_preds))\n",
    "\n",
    "    print(f\"Evaluating {conv1d_model.model_name}\")\n",
    "    conv1d_labels, conv1d_preds = conv1d_model.evaluate(valid_dataloader_seq, config)\n",
    "    model_fold_results[conv1d_model.model_name][\"Labels\"].extend(list(conv1d_labels))\n",
    "    model_fold_results[conv1d_model.model_name][\"Preds\"].extend(list(conv1d_preds))\n",
    "\n",
    "    print(f\"Evaluating {trs_model.model_name}\")\n",
    "    trs_labels, trs_preds = trs_model.evaluate(valid_dataloader_trs, config)\n",
    "    model_fold_results[trs_model.model_name][\"Labels\"].extend(list(trs_labels))\n",
    "    model_fold_results[trs_model.model_name][\"Preds\"].extend(list(trs_preds))\n",
    "kcm_atb_model_fold_results = model_fold_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ATB Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare baseline models\n",
    "avg_model = data_utils.load_pkl(f\"{run_folder}atb/models/AVG_{FOLD_MODEL}.pkl\")\n",
    "sch_model = data_utils.load_pkl(f\"{run_folder}atb/models/SCH_{FOLD_MODEL}.pkl\")\n",
    "tim_model = data_utils.load_pkl(f\"{run_folder}atb/models/PER_TIM_{FOLD_MODEL}.pkl\")\n",
    "\n",
    "# Declare network models\n",
    "ff_model = ff.FF(\n",
    "    \"FF\",\n",
    "    11,\n",
    "    HIDDEN_SIZE,\n",
    "    BATCH_SIZE,\n",
    "    embed_dict,\n",
    "    device\n",
    ").to(device)\n",
    "ff_model.load_state_dict(torch.load(f\"{run_folder}atb/models/{ff_model.model_name}_{FOLD_MODEL}.pt\"))\n",
    "print(ff_model.model_name)\n",
    "print(sum(p.numel() for p in ff_model.parameters() if p.requires_grad))\n",
    "gru_model = rnn.GRU_RNN(\n",
    "    \"GRU_RNN\",\n",
    "    8,\n",
    "    1,\n",
    "    HIDDEN_SIZE,\n",
    "    BATCH_SIZE,\n",
    "    embed_dict,\n",
    "    device\n",
    ").to(device)\n",
    "gru_model.load_state_dict(torch.load(f\"{run_folder}atb/models/{gru_model.model_name}_{FOLD_MODEL}.pt\"))\n",
    "print(gru_model.model_name)\n",
    "print(sum(p.numel() for p in gru_model.parameters() if p.requires_grad))\n",
    "gru_mto_model = rnn.GRU_RNN_MTO(\n",
    "    \"GRU_RNN_MTO\",\n",
    "    8,\n",
    "    1,\n",
    "    HIDDEN_SIZE,\n",
    "    BATCH_SIZE,\n",
    "    embed_dict,\n",
    "    device\n",
    ").to(device)\n",
    "gru_mto_model.load_state_dict(torch.load(f\"{run_folder}atb/models/{gru_mto_model.model_name}_{FOLD_MODEL}.pt\"))\n",
    "print(gru_mto_model.model_name)\n",
    "print(sum(p.numel() for p in gru_mto_model.parameters() if p.requires_grad))\n",
    "conv1d_model = conv.CONV(\n",
    "    \"CONV1D\",\n",
    "    8,\n",
    "    1,\n",
    "    HIDDEN_SIZE,\n",
    "    BATCH_SIZE,\n",
    "    embed_dict,\n",
    "    device\n",
    ").to(device)\n",
    "conv1d_model.load_state_dict(torch.load(f\"{run_folder}atb/models/{conv1d_model.model_name}_{FOLD_MODEL}.pt\"))\n",
    "print(conv1d_model.model_name)\n",
    "print(sum(p.numel() for p in conv1d_model.parameters() if p.requires_grad))\n",
    "trs_model = transformer.TRANSFORMER(\n",
    "    \"TRSF_ENC\",\n",
    "    8,\n",
    "    1,\n",
    "    HIDDEN_SIZE,\n",
    "    BATCH_SIZE,\n",
    "    embed_dict,\n",
    "    device\n",
    ").to(device)\n",
    "trs_model.load_state_dict(torch.load(f\"{run_folder}atb/models/{trs_model.model_name}_{FOLD_MODEL}.pt\"))\n",
    "print(trs_model.model_name)\n",
    "print(sum(p.numel() for p in trs_model.parameters() if p.requires_grad))\n",
    "\n",
    "# Add all models to results list\n",
    "model_list = []\n",
    "model_list.append(avg_model)\n",
    "model_list.append(sch_model)\n",
    "model_list.append(tim_model)\n",
    "model_list.append(ff_model)\n",
    "model_list.append(gru_model)\n",
    "model_list.append(gru_mto_model)\n",
    "model_list.append(conv1d_model)\n",
    "model_list.append(trs_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Evaluating ATB on ATB\")\n",
    "model_fold_results = {}\n",
    "for x in model_list:\n",
    "    model_fold_results[x.model_name] = {\"Labels\":[], \"Preds\":[]}\n",
    "for valid_file in test_file_list_atb:\n",
    "    print(f\"VALIDATE ON FILE: {valid_file}\")\n",
    "\n",
    "    # These are fold holdouts, separate validation files are used for generalization\n",
    "    valid_data = data_utils.load_all_data(atb_data_folder, valid_file)\n",
    "    with open(f\"{atb_data_folder}train_config.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # Construct dataloaders for network models\n",
    "    valid_dataloader_basic = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_collate, NUM_WORKERS)\n",
    "    valid_dataloader_seq = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_collate, NUM_WORKERS)\n",
    "    valid_dataloader_seq_mto = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_mto_collate, NUM_WORKERS)\n",
    "    valid_dataloader_trs = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.transformer_collate, NUM_WORKERS)\n",
    "    print(f\"Successfully loaded {len(valid_data)} testing samples.\")\n",
    "\n",
    "    print(f\"Evaluating {avg_model.model_name}\")\n",
    "    avg_labels, avg_preds = avg_model.predict(valid_dataloader_basic, config)\n",
    "    model_fold_results[avg_model.model_name][\"Labels\"].extend(list(avg_labels))\n",
    "    model_fold_results[avg_model.model_name][\"Preds\"].extend(list(avg_preds))\n",
    "\n",
    "    print(f\"Evaluating {sch_model.model_name}\")\n",
    "    sch_labels, sch_preds = sch_model.predict(valid_dataloader_basic, config)\n",
    "    model_fold_results[sch_model.model_name][\"Labels\"].extend(list(sch_labels))\n",
    "    model_fold_results[sch_model.model_name][\"Preds\"].extend(list(sch_preds))\n",
    "\n",
    "    print(f\"Evaluating {tim_model.model_name}\")\n",
    "    tim_labels, tim_preds = tim_model.predict(valid_dataloader_seq, config)\n",
    "    model_fold_results[tim_model.model_name][\"Labels\"].extend(list(tim_labels))\n",
    "    model_fold_results[tim_model.model_name][\"Preds\"].extend(list(tim_preds))\n",
    "\n",
    "    print(f\"Evaluating {ff_model.model_name}\")\n",
    "    ff_labels, ff_preds = ff_model.evaluate(valid_dataloader_basic, config)\n",
    "    model_fold_results[ff_model.model_name][\"Labels\"].extend(list(ff_labels))\n",
    "    model_fold_results[ff_model.model_name][\"Preds\"].extend(list(ff_preds))\n",
    "\n",
    "    print(f\"Evaluating {gru_model.model_name}\")\n",
    "    gru_labels, gru_preds = gru_model.evaluate(valid_dataloader_seq, config)\n",
    "    model_fold_results[gru_model.model_name][\"Labels\"].extend(list(gru_labels))\n",
    "    model_fold_results[gru_model.model_name][\"Preds\"].extend(list(gru_preds))\n",
    "\n",
    "    print(f\"Evaluating {gru_mto_model.model_name}\")\n",
    "    gru_mto_labels, gru_mto_preds = gru_mto_model.evaluate(valid_dataloader_seq_mto, config)\n",
    "    model_fold_results[gru_mto_model.model_name][\"Labels\"].extend(list(gru_mto_labels))\n",
    "    model_fold_results[gru_mto_model.model_name][\"Preds\"].extend(list(gru_mto_preds))\n",
    "\n",
    "    print(f\"Evaluating {conv1d_model.model_name}\")\n",
    "    conv1d_labels, conv1d_preds = conv1d_model.evaluate(valid_dataloader_seq, config)\n",
    "    model_fold_results[conv1d_model.model_name][\"Labels\"].extend(list(conv1d_labels))\n",
    "    model_fold_results[conv1d_model.model_name][\"Preds\"].extend(list(conv1d_preds))\n",
    "\n",
    "    print(f\"Evaluating {trs_model.model_name}\")\n",
    "    trs_labels, trs_preds = trs_model.evaluate(valid_dataloader_trs, config)\n",
    "    model_fold_results[trs_model.model_name][\"Labels\"].extend(list(trs_labels))\n",
    "    model_fold_results[trs_model.model_name][\"Preds\"].extend(list(trs_preds))\n",
    "atb_atb_model_fold_results = model_fold_results\n",
    "\n",
    "print(f\"Evaluating ATB on KCM\")\n",
    "model_fold_results = {}\n",
    "for x in model_list:\n",
    "    model_fold_results[x.model_name] = {\"Labels\":[], \"Preds\":[]}\n",
    "for valid_file in test_file_list_kcm:\n",
    "    print(f\"VALIDATE ON FILE: {valid_file}\")\n",
    "\n",
    "    # These are fold holdouts, separate validation files are used for generalization\n",
    "    valid_data = data_utils.load_all_data(kcm_data_folder, valid_file)\n",
    "    with open(f\"{kcm_data_folder}train_config.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # Construct dataloaders for network models\n",
    "    valid_dataloader_basic = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_collate, NUM_WORKERS)\n",
    "    valid_dataloader_seq = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_collate, NUM_WORKERS)\n",
    "    valid_dataloader_seq_mto = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_mto_collate, NUM_WORKERS)\n",
    "    valid_dataloader_trs = data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.transformer_collate, NUM_WORKERS)\n",
    "    print(f\"Successfully loaded {len(valid_data)} testing samples.\")\n",
    "\n",
    "    print(f\"Evaluating {avg_model.model_name}\")\n",
    "    avg_labels, avg_preds = avg_model.predict(valid_dataloader_basic, config)\n",
    "    model_fold_results[avg_model.model_name][\"Labels\"].extend(list(avg_labels))\n",
    "    model_fold_results[avg_model.model_name][\"Preds\"].extend(list(avg_preds))\n",
    "\n",
    "    print(f\"Evaluating {sch_model.model_name}\")\n",
    "    sch_labels, sch_preds = sch_model.predict(valid_dataloader_basic, config)\n",
    "    model_fold_results[sch_model.model_name][\"Labels\"].extend(list(sch_labels))\n",
    "    model_fold_results[sch_model.model_name][\"Preds\"].extend(list(sch_preds))\n",
    "\n",
    "    print(f\"Evaluating {tim_model.model_name}\")\n",
    "    tim_labels, tim_preds = tim_model.predict(valid_dataloader_seq, config)\n",
    "    model_fold_results[tim_model.model_name][\"Labels\"].extend(list(tim_labels))\n",
    "    model_fold_results[tim_model.model_name][\"Preds\"].extend(list(tim_preds))\n",
    "\n",
    "    print(f\"Evaluating {ff_model.model_name}\")\n",
    "    ff_labels, ff_preds = ff_model.evaluate(valid_dataloader_basic, config)\n",
    "    model_fold_results[ff_model.model_name][\"Labels\"].extend(list(ff_labels))\n",
    "    model_fold_results[ff_model.model_name][\"Preds\"].extend(list(ff_preds))\n",
    "\n",
    "    print(f\"Evaluating {gru_model.model_name}\")\n",
    "    gru_labels, gru_preds = gru_model.evaluate(valid_dataloader_seq, config)\n",
    "    model_fold_results[gru_model.model_name][\"Labels\"].extend(list(gru_labels))\n",
    "    model_fold_results[gru_model.model_name][\"Preds\"].extend(list(gru_preds))\n",
    "\n",
    "    print(f\"Evaluating {gru_mto_model.model_name}\")\n",
    "    gru_mto_labels, gru_mto_preds = gru_mto_model.evaluate(valid_dataloader_seq_mto, config)\n",
    "    model_fold_results[gru_mto_model.model_name][\"Labels\"].extend(list(gru_mto_labels))\n",
    "    model_fold_results[gru_mto_model.model_name][\"Preds\"].extend(list(gru_mto_preds))\n",
    "\n",
    "    print(f\"Evaluating {conv1d_model.model_name}\")\n",
    "    conv1d_labels, conv1d_preds = conv1d_model.evaluate(valid_dataloader_seq, config)\n",
    "    model_fold_results[conv1d_model.model_name][\"Labels\"].extend(list(conv1d_labels))\n",
    "    model_fold_results[conv1d_model.model_name][\"Preds\"].extend(list(conv1d_preds))\n",
    "\n",
    "    print(f\"Evaluating {trs_model.model_name}\")\n",
    "    trs_labels, trs_preds = trs_model.evaluate(valid_dataloader_trs, config)\n",
    "    model_fold_results[trs_model.model_name][\"Labels\"].extend(list(trs_labels))\n",
    "    model_fold_results[trs_model.model_name][\"Preds\"].extend(list(trs_preds))\n",
    "atb_kcm_model_fold_results = model_fold_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "train_nets = []\n",
    "test_nets = []\n",
    "errors = []\n",
    "results = kcm_kcm_model_fold_results\n",
    "for model_name in results.keys():\n",
    "    models.append(model_name)\n",
    "    train_nets.append(\"KCM\")\n",
    "    test_nets.append(\"KCM\")\n",
    "    errors.append(np.round(np.sqrt(metrics.mean_squared_error(results[model_name]['Labels'], results[model_name]['Preds'])), 2))\n",
    "results = kcm_atb_model_fold_results\n",
    "for model_name in results.keys():\n",
    "    models.append(model_name)\n",
    "    train_nets.append(\"KCM\")\n",
    "    test_nets.append(\"ATB\")\n",
    "    errors.append(np.round(np.sqrt(metrics.mean_squared_error(results[model_name]['Labels'], results[model_name]['Preds'])), 2))\n",
    "results = atb_atb_model_fold_results\n",
    "for model_name in results.keys():\n",
    "    models.append(model_name)\n",
    "    train_nets.append(\"ATB\")\n",
    "    test_nets.append(\"ATB\")\n",
    "    errors.append(np.round(np.sqrt(metrics.mean_squared_error(results[model_name]['Labels'], results[model_name]['Preds'])), 2))\n",
    "results = atb_kcm_model_fold_results\n",
    "for model_name in results.keys():\n",
    "    models.append(model_name)\n",
    "    train_nets.append(\"ATB\")\n",
    "    test_nets.append(\"KCM\")\n",
    "    errors.append(np.round(np.sqrt(metrics.mean_squared_error(results[model_name]['Labels'], results[model_name]['Preds'])), 2))\n",
    "\n",
    "metric_results = pd.DataFrame(columns=[\"Model\",\"Train Network\",\"Test Network\",\"Metric\"])\n",
    "metric_results['Model'] = models\n",
    "metric_results['Train Network'] = train_nets\n",
    "metric_results['Test Network'] = test_nets\n",
    "metric_results['Metric'] = errors\n",
    "metric_results['Model-Train-Test'] = metric_results['Model']+\"_\"+metric_results['Train Network']+\"_\"+metric_results['Test Network']\n",
    "metric_results = metric_results.sort_values('Model-Train-Test')\n",
    "metric_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include DeepTTE Generalization Results\n",
    "kcm_deeptte_gen_res = data_utils.extract_deeptte_results(\"KCM\", run_folder_tte, \"kcm/\", generalization_flag=True)\n",
    "atb_deeptte_gen_res = data_utils.extract_deeptte_results(\"ATB\", run_folder_tte, \"atb/\", generalization_flag=True)\n",
    "deeptte_gen_res = pd.concat([kcm_deeptte_gen_res, atb_deeptte_gen_res])\n",
    "deeptte_gen_res[\"Model\"] = \"DEEPTTE\"\n",
    "deeptte_gen_res[\"Train Network\"] = deeptte_gen_res[\"City\"]\n",
    "deeptte_gen_res[\"Test Network\"] = deeptte_gen_res[\"Loss Set\"]\n",
    "deeptte_gen_res[\"Metric\"] = deeptte_gen_res[\"RMSE\"]\n",
    "deeptte_gen_res = deeptte_gen_res[[\"Model\",\"Train Network\",\"Test Network\",\"Metric\"]]\n",
    "deeptte_gen_res['Model-Train-Test'] = deeptte_gen_res['Model']+\"_\"+deeptte_gen_res['Train Network']+\"_\"+deeptte_gen_res['Test Network']\n",
    "metric_results = pd.concat([metric_results, deeptte_gen_res])\n",
    "metric_results = metric_results.sort_values('Model-Train-Test')\n",
    "metric_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = metric_results[metric_results['Test Network']==\"KCM\"]\n",
    "fig, axes = plt.subplots(1,1)\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(6)\n",
    "sns.barplot(plot_data, x=\"Metric\", y=\"Model-Train-Test\", hue=\"Model\", dodge=False)\n",
    "axes.set_ylabel(\"Model\")\n",
    "axes.set_xlabel(\"RMSE\")\n",
    "fig.suptitle('Model Generalization Performance On KCM', fontsize=16)\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"../plots/model_generalization.png\", dpi=1800, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = metric_results[metric_results['Test Network']==\"ATB\"]\n",
    "fig, axes = plt.subplots(1,1)\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(6)\n",
    "sns.barplot(plot_data, x=\"Metric\", y=\"Model-Train-Test\", hue=\"Model\", dodge=False)\n",
    "axes.set_ylabel(\"Model\")\n",
    "axes.set_xlabel(\"RMSE\")\n",
    "fig.suptitle('Model Generalization Performance On ATB', fontsize=16)\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"../plots/model_generalization.png\", dpi=1800, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "valle_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b30fe1de1713ca8e7537eef068b13a2de77ded03f86aab2e80ea73416dd3d704"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
