{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import importlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "\n",
    "from models import avg_speed, schedule, ff, persistent, rnn, transformer, conv\n",
    "from utils import data_utils, data_loader, model_utils, shape_utils\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Get list of available train/test files for chosen run folder\n",
    "run_folder_kcm = \"../results/debug/\"\n",
    "run_folder_atb = \"../results/debug/\"\n",
    "run_folder_tte = \"../results/debug/\"\n",
    "kcm_data_folder = f\"{run_folder_kcm}kcm/deeptte_formatted/\"\n",
    "atb_data_folder = f\"{run_folder_atb}atb/deeptte_formatted/\"\n",
    "train_file_list_kcm = list(filter(lambda x: x[:5]==\"train\" and len(x)==6, os.listdir(kcm_data_folder)))\n",
    "test_file_list_kcm = list(filter(lambda x: x[:4]==\"test\" and len(x)==5, os.listdir(kcm_data_folder)))\n",
    "train_file_list_atb = list(filter(lambda x: x[:5]==\"train\" and len(x)==6, os.listdir(atb_data_folder)))\n",
    "test_file_list_atb = list(filter(lambda x: x[:4]==\"test\" and len(x)==5, os.listdir(atb_data_folder)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device to train on, and number workers if GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    NUM_WORKERS = 8\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    NUM_WORKERS = 0\n",
    "print(f\"DEVICE: {device}\")\n",
    "print(f\"WORKERS: {NUM_WORKERS}\")\n",
    "\n",
    "# Set hyperparameters\n",
    "HIDDEN_SIZE = 32\n",
    "BATCH_SIZE = 512\n",
    "FOLD_MODEL = 0\n",
    "\n",
    "# Define embedded variables for network models\n",
    "embed_dict = {\n",
    "    'timeID': {\n",
    "        'vocab_size': 1440,\n",
    "        'embed_dims': 24\n",
    "    },\n",
    "    'weekID': {\n",
    "        'vocab_size': 7,\n",
    "        'embed_dims': 4\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load KCM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare baseline models\n",
    "avg_model = data_utils.load_pkl(f\"{run_folder_kcm}kcm/models/AVG_{FOLD_MODEL}.pkl\")\n",
    "sch_model = data_utils.load_pkl(f\"{run_folder_kcm}kcm/models/SCH_{FOLD_MODEL}.pkl\")\n",
    "tim_model = data_utils.load_pkl(f\"{run_folder_kcm}kcm/models/PER_TIM_{FOLD_MODEL}.pkl\")\n",
    "\n",
    "# Declare network models\n",
    "ff_model = ff.FF(\n",
    "    \"FF\",\n",
    "    n_features=11,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    embed_dict=embed_dict,\n",
    "    device=device\n",
    ").to(device)\n",
    "ff_grid_model1 = ff.FF_GRID(\n",
    "    \"FF_GRID_IND\",\n",
    "    n_features=11,\n",
    "    n_grid_features=8*3*3,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    grid_compression_size=8,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    embed_dict=embed_dict,\n",
    "    device=device\n",
    ").to(device)\n",
    "ff_grid_model2 = ff.FF_GRID_ATTN(\n",
    "    \"FF_GRID_ATTN\",\n",
    "    n_features=11,\n",
    "    n_grid_features=8*3*3,\n",
    "    n_channels=8,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    grid_compression_size=8,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    embed_dict=embed_dict,\n",
    "    device=device\n",
    ").to(device)\n",
    "ff_grid_model3 = ff.FF_GRID(\n",
    "    \"FF_NGRID_IND\",\n",
    "    n_features=11,\n",
    "    n_grid_features=4*3*3*3,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    grid_compression_size=8,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    embed_dict=embed_dict,\n",
    "    device=device\n",
    ").to(device)\n",
    "gru_model = rnn.GRU(\n",
    "    \"GRU\",\n",
    "    n_features=8,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    embed_dict=embed_dict,\n",
    "    device=device\n",
    ").to(device)\n",
    "gru_grid_model1 = rnn.GRU_GRID(\n",
    "    \"GRU_GRID_IND\",\n",
    "    n_features=8,\n",
    "    n_grid_features=8*3*3,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    grid_compression_size=8,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    embed_dict=embed_dict,\n",
    "    device=device\n",
    ").to(device)\n",
    "gru_grid_model2 = rnn.GRU_GRID_ATTN(\n",
    "    \"GRU_GRID_ATTN\",\n",
    "    n_features=8,\n",
    "    n_grid_features=8*3*3,\n",
    "    n_channels=8,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    grid_compression_size=8,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    embed_dict=embed_dict,\n",
    "    device=device\n",
    ").to(device)\n",
    "gru_grid_model3 = rnn.GRU_GRID(\n",
    "    \"GRU_NGRID_IND\",\n",
    "    n_features=8,\n",
    "    n_grid_features=4*3*3*3,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    grid_compression_size=8,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    embed_dict=embed_dict,\n",
    "    device=device\n",
    ").to(device)\n",
    "trs_model = transformer.TRSF(\n",
    "    \"TRSF_ENC\",\n",
    "    n_features=8,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    embed_dict=embed_dict,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# Add all models to results list\n",
    "base_model_list = []\n",
    "base_model_list.append(avg_model)\n",
    "base_model_list.append(sch_model)\n",
    "base_model_list.append(tim_model)\n",
    "nn_model_list = []\n",
    "nn_model_list.append(ff_model)\n",
    "nn_model_list.append(ff_grid_model1)\n",
    "nn_model_list.append(ff_grid_model2)\n",
    "nn_model_list.append(ff_grid_model3)\n",
    "nn_model_list.append(gru_model)\n",
    "nn_model_list.append(gru_grid_model1)\n",
    "nn_model_list.append(gru_grid_model2)\n",
    "nn_model_list.append(gru_grid_model3)\n",
    "# nn_model_list.append(gru_mto_model)\n",
    "# nn_model_list.append(conv1d_model)\n",
    "nn_model_list.append(trs_model)\n",
    "all_model_list = []\n",
    "all_model_list.extend(base_model_list)\n",
    "all_model_list.extend(nn_model_list)\n",
    "\n",
    "# Load all model weights\n",
    "for m in nn_model_list:\n",
    "    m = m.load_state_dict(torch.load(f\"{run_folder_kcm}kcm/models/{m.model_name}_{FOLD_MODEL}.pt\"))\n",
    "\n",
    "print(f\"Model names: {[m.model_name for m in nn_model_list]}\")\n",
    "print(f\"Model total parameters: {[sum(p.numel() for p in m.parameters()) for m in nn_model_list]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Evaluating KCM on KCM\")\n",
    "model_fold_results = {}\n",
    "for x in all_model_list:\n",
    "    model_fold_results[x.model_name] = {\"Labels\":[], \"Preds\":[]}\n",
    "\n",
    "for valid_file in test_file_list_kcm:\n",
    "    # These are fold holdouts, separate validation files are used for generalization\n",
    "    valid_data, grid, ngrid = data_utils.load_all_data(kcm_data_folder, valid_file)\n",
    "    print(f\"Successfully loaded {len(valid_data)} testing samples.\")\n",
    "    grid_content = grid.get_fill_content()\n",
    "    print(f\"VALIDATE FILE: {valid_file}\")\n",
    "    with open(f\"{kcm_data_folder}train_config.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # Construct dataloaders for all models\n",
    "    dataloaders = []\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_collate, NUM_WORKERS))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_collate, NUM_WORKERS))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_collate, NUM_WORKERS))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_collate, NUM_WORKERS))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_grid_collate, NUM_WORKERS, grid=grid_content, buffer=1))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_grid_collate, NUM_WORKERS, grid=grid_content, buffer=1))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_grid_collate, NUM_WORKERS, grid=ngrid, is_ngrid=True, buffer=1))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_collate, NUM_WORKERS))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_grid_collate, NUM_WORKERS, grid=grid_content, buffer=1))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_grid_collate, NUM_WORKERS, grid=grid_content, buffer=1))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_grid_collate, NUM_WORKERS, grid=ngrid, is_ngrid=True, buffer=1))\n",
    "    # dataloaders.append(data_loader.make_generic_dataloader(test_data, config, BATCH_SIZE, data_loader.sequential_mto_collate, NUM_WORKERS))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.transformer_collate, NUM_WORKERS))\n",
    "\n",
    "    # Test all models\n",
    "    for model, loader in zip(all_model_list, dataloaders):\n",
    "        print(f\"Evaluating: {model.model_name}\")\n",
    "        labels, preds = model.evaluate(loader, config)\n",
    "        model_fold_results[model.model_name][\"Labels\"].extend(list(labels))\n",
    "        model_fold_results[model.model_name][\"Preds\"].extend(list(preds))\n",
    "kcm_kcm_model_fold_results = model_fold_results\n",
    "\n",
    "print(f\"Evaluating KCM on ATB\")\n",
    "model_fold_results = {}\n",
    "for x in all_model_list:\n",
    "    model_fold_results[x.model_name] = {\"Labels\":[], \"Preds\":[]}\n",
    "\n",
    "for valid_file in test_file_list_atb:\n",
    "    # These are fold holdouts, separate validation files are used for generalization\n",
    "    valid_data, grid, ngrid = data_utils.load_all_data(atb_data_folder, valid_file)\n",
    "    print(f\"Successfully loaded {len(valid_data)} testing samples.\")\n",
    "    grid_content = grid.get_fill_content()\n",
    "    print(f\"VALIDATE FILE: {valid_file}\")\n",
    "    with open(f\"{kcm_data_folder}train_config.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # Construct dataloaders for all models\n",
    "    dataloaders = []\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_collate, NUM_WORKERS))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_collate, NUM_WORKERS))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_collate, NUM_WORKERS))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_collate, NUM_WORKERS))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_grid_collate, NUM_WORKERS, grid=grid_content, buffer=1))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_grid_collate, NUM_WORKERS, grid=grid_content, buffer=1))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_grid_collate, NUM_WORKERS, grid=ngrid, is_ngrid=True, buffer=1))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_collate, NUM_WORKERS))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_grid_collate, NUM_WORKERS, grid=grid_content, buffer=1))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_grid_collate, NUM_WORKERS, grid=grid_content, buffer=1))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_grid_collate, NUM_WORKERS, grid=ngrid, is_ngrid=True, buffer=1))\n",
    "    # dataloaders.append(data_loader.make_generic_dataloader(test_data, config, BATCH_SIZE, data_loader.sequential_mto_collate, NUM_WORKERS))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.transformer_collate, NUM_WORKERS))\n",
    "\n",
    "    # Test all models\n",
    "    for model, loader in zip(all_model_list, dataloaders):\n",
    "        labels, preds = model.evaluate(loader, config)\n",
    "        model_fold_results[model.model_name][\"Labels\"].extend(list(labels))\n",
    "        model_fold_results[model.model_name][\"Preds\"].extend(list(preds))\n",
    "kcm_atb_model_fold_results = model_fold_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ATB Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare baseline models\n",
    "avg_model = data_utils.load_pkl(f\"{run_folder_atb}atb/models/AVG_{FOLD_MODEL}.pkl\")\n",
    "sch_model = data_utils.load_pkl(f\"{run_folder_atb}atb/models/SCH_{FOLD_MODEL}.pkl\")\n",
    "tim_model = data_utils.load_pkl(f\"{run_folder_atb}atb/models/PER_TIM_{FOLD_MODEL}.pkl\")\n",
    "\n",
    "# Declare network models\n",
    "ff_model = ff.FF(\n",
    "    \"FF\",\n",
    "    n_features=11,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    embed_dict=embed_dict,\n",
    "    device=device\n",
    ").to(device)\n",
    "ff_grid_model1 = ff.FF_GRID(\n",
    "    \"FF_GRID_IND\",\n",
    "    n_features=11,\n",
    "    n_grid_features=8*3*3,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    grid_compression_size=8,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    embed_dict=embed_dict,\n",
    "    device=device\n",
    ").to(device)\n",
    "ff_grid_model2 = ff.FF_GRID_ATTN(\n",
    "    \"FF_GRID_ATTN\",\n",
    "    n_features=11,\n",
    "    n_grid_features=8*3*3,\n",
    "    n_channels=8,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    grid_compression_size=8,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    embed_dict=embed_dict,\n",
    "    device=device\n",
    ").to(device)\n",
    "ff_grid_model3 = ff.FF_GRID(\n",
    "    \"FF_NGRID_IND\",\n",
    "    n_features=11,\n",
    "    n_grid_features=4*3*3*3,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    grid_compression_size=8,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    embed_dict=embed_dict,\n",
    "    device=device\n",
    ").to(device)\n",
    "gru_model = rnn.GRU(\n",
    "    \"GRU\",\n",
    "    n_features=8,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    embed_dict=embed_dict,\n",
    "    device=device\n",
    ").to(device)\n",
    "gru_grid_model1 = rnn.GRU_GRID(\n",
    "    \"GRU_GRID_IND\",\n",
    "    n_features=8,\n",
    "    n_grid_features=8*3*3,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    grid_compression_size=8,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    embed_dict=embed_dict,\n",
    "    device=device\n",
    ").to(device)\n",
    "gru_grid_model2 = rnn.GRU_GRID_ATTN(\n",
    "    \"GRU_GRID_ATTN\",\n",
    "    n_features=8,\n",
    "    n_grid_features=8*3*3,\n",
    "    n_channels=8,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    grid_compression_size=8,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    embed_dict=embed_dict,\n",
    "    device=device\n",
    ").to(device)\n",
    "gru_grid_model3 = rnn.GRU_GRID(\n",
    "    \"GRU_NGRID_IND\",\n",
    "    n_features=8,\n",
    "    n_grid_features=4*3*3*3,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    grid_compression_size=8,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    embed_dict=embed_dict,\n",
    "    device=device\n",
    ").to(device)\n",
    "trs_model = transformer.TRSF(\n",
    "    \"TRSF_ENC\",\n",
    "    n_features=8,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    embed_dict=embed_dict,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# Add all models to results list\n",
    "base_model_list = []\n",
    "base_model_list.append(avg_model)\n",
    "base_model_list.append(sch_model)\n",
    "base_model_list.append(tim_model)\n",
    "nn_model_list = []\n",
    "nn_model_list.append(ff_model)\n",
    "nn_model_list.append(ff_grid_model1)\n",
    "nn_model_list.append(ff_grid_model2)\n",
    "nn_model_list.append(ff_grid_model3)\n",
    "nn_model_list.append(gru_model)\n",
    "nn_model_list.append(gru_grid_model1)\n",
    "nn_model_list.append(gru_grid_model2)\n",
    "nn_model_list.append(gru_grid_model3)\n",
    "# nn_model_list.append(gru_mto_model)\n",
    "# nn_model_list.append(conv1d_model)\n",
    "nn_model_list.append(trs_model)\n",
    "all_model_list = []\n",
    "all_model_list.extend(base_model_list)\n",
    "all_model_list.extend(nn_model_list)\n",
    "\n",
    "# Load all model weights\n",
    "for m in nn_model_list:\n",
    "    m = m.load_state_dict(torch.load(f\"{run_folder_atb}atb/models/{m.model_name}_{FOLD_MODEL}.pt\"))\n",
    "\n",
    "print(f\"Model names: {[m.model_name for m in nn_model_list]}\")\n",
    "print(f\"Model total parameters: {[sum(p.numel() for p in m.parameters()) for m in nn_model_list]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Evaluating AtB on AtB\")\n",
    "model_fold_results = {}\n",
    "for x in all_model_list:\n",
    "    model_fold_results[x.model_name] = {\"Labels\":[], \"Preds\":[]}\n",
    "\n",
    "for valid_file in test_file_list_atb:\n",
    "    # These are fold holdouts, separate validation files are used for generalization\n",
    "    valid_data, grid, ngrid = data_utils.load_all_data(atb_data_folder, valid_file)\n",
    "    print(f\"Successfully loaded {len(valid_data)} testing samples.\")\n",
    "    grid_content = grid.get_fill_content()\n",
    "    print(f\"VALIDATE FILE: {valid_file}\")\n",
    "    with open(f\"{atb_data_folder}train_config.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # Construct dataloaders for all models\n",
    "    dataloaders = []\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_collate, NUM_WORKERS))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_collate, NUM_WORKERS))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_collate, NUM_WORKERS))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_collate, NUM_WORKERS))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_grid_collate, NUM_WORKERS, grid=grid_content, buffer=1))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_grid_collate, NUM_WORKERS, grid=grid_content, buffer=1))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_grid_collate, NUM_WORKERS, grid=ngrid, is_ngrid=True, buffer=1))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_collate, NUM_WORKERS))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_grid_collate, NUM_WORKERS, grid=grid_content, buffer=1))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_grid_collate, NUM_WORKERS, grid=grid_content, buffer=1))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_grid_collate, NUM_WORKERS, grid=ngrid, is_ngrid=True, buffer=1))\n",
    "    # dataloaders.append(data_loader.make_generic_dataloader(test_data, config, BATCH_SIZE, data_loader.sequential_mto_collate, NUM_WORKERS))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.transformer_collate, NUM_WORKERS))\n",
    "\n",
    "    # Test all models\n",
    "    for model, loader in zip(all_model_list, dataloaders):\n",
    "        print(f\"Evaluating: {model.model_name}\")\n",
    "        labels, preds = model.evaluate(loader, config)\n",
    "        model_fold_results[model.model_name][\"Labels\"].extend(list(labels))\n",
    "        model_fold_results[model.model_name][\"Preds\"].extend(list(preds))\n",
    "atb_atb_model_fold_results = model_fold_results\n",
    "\n",
    "print(f\"Evaluating AtB on KCM\")\n",
    "model_fold_results = {}\n",
    "for x in all_model_list:\n",
    "    model_fold_results[x.model_name] = {\"Labels\":[], \"Preds\":[]}\n",
    "\n",
    "for valid_file in test_file_list_kcm:\n",
    "    # These are fold holdouts, separate validation files are used for generalization\n",
    "    valid_data, grid, ngrid = data_utils.load_all_data(kcm_data_folder, valid_file)\n",
    "    print(f\"Successfully loaded {len(valid_data)} testing samples.\")\n",
    "    grid_content = grid.get_fill_content()\n",
    "    print(f\"VALIDATE FILE: {valid_file}\")\n",
    "    with open(f\"{atb_data_folder}train_config.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # Construct dataloaders for all models\n",
    "    dataloaders = []\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_collate, NUM_WORKERS))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_collate, NUM_WORKERS))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_collate, NUM_WORKERS))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_collate, NUM_WORKERS))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_grid_collate, NUM_WORKERS, grid=grid_content, buffer=1))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_grid_collate, NUM_WORKERS, grid=grid_content, buffer=1))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.basic_grid_collate, NUM_WORKERS, grid=ngrid, is_ngrid=True, buffer=1))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_collate, NUM_WORKERS))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_grid_collate, NUM_WORKERS, grid=grid_content, buffer=1))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_grid_collate, NUM_WORKERS, grid=grid_content, buffer=1))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.sequential_grid_collate, NUM_WORKERS, grid=ngrid, is_ngrid=True, buffer=1))\n",
    "    # dataloaders.append(data_loader.make_generic_dataloader(test_data, config, BATCH_SIZE, data_loader.sequential_mto_collate, NUM_WORKERS))\n",
    "    dataloaders.append(data_loader.make_generic_dataloader(valid_data, config, BATCH_SIZE, data_loader.transformer_collate, NUM_WORKERS))\n",
    "\n",
    "    # Test all models\n",
    "    for model, loader in zip(all_model_list, dataloaders):\n",
    "        labels, preds = model.evaluate(loader, config)\n",
    "        model_fold_results[model.model_name][\"Labels\"].extend(list(labels))\n",
    "        model_fold_results[model.model_name][\"Preds\"].extend(list(preds))\n",
    "atb_kcm_model_fold_results = model_fold_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "train_nets = []\n",
    "test_nets = []\n",
    "errors = []\n",
    "results = kcm_kcm_model_fold_results\n",
    "for model_name in results.keys():\n",
    "    models.append(model_name)\n",
    "    train_nets.append(\"KCM\")\n",
    "    test_nets.append(\"KCM\")\n",
    "    errors.append(np.round(np.sqrt(metrics.mean_squared_error(results[model_name]['Labels'], results[model_name]['Preds'])), 2))\n",
    "results = kcm_atb_model_fold_results\n",
    "for model_name in results.keys():\n",
    "    models.append(model_name)\n",
    "    train_nets.append(\"KCM\")\n",
    "    test_nets.append(\"ATB\")\n",
    "    errors.append(np.round(np.sqrt(metrics.mean_squared_error(results[model_name]['Labels'], results[model_name]['Preds'])), 2))\n",
    "results = atb_atb_model_fold_results\n",
    "for model_name in results.keys():\n",
    "    models.append(model_name)\n",
    "    train_nets.append(\"ATB\")\n",
    "    test_nets.append(\"ATB\")\n",
    "    errors.append(np.round(np.sqrt(metrics.mean_squared_error(results[model_name]['Labels'], results[model_name]['Preds'])), 2))\n",
    "results = atb_kcm_model_fold_results\n",
    "for model_name in results.keys():\n",
    "    models.append(model_name)\n",
    "    train_nets.append(\"ATB\")\n",
    "    test_nets.append(\"KCM\")\n",
    "    errors.append(np.round(np.sqrt(metrics.mean_squared_error(results[model_name]['Labels'], results[model_name]['Preds'])), 2))\n",
    "\n",
    "metric_results = pd.DataFrame(columns=[\"Model\",\"Train Network\",\"Test Network\",\"Metric\"])\n",
    "metric_results['Model'] = models\n",
    "metric_results['Train Network'] = train_nets\n",
    "metric_results['Test Network'] = test_nets\n",
    "metric_results['Metric'] = errors\n",
    "metric_results['Model-Train-Test'] = metric_results['Model']+\"_\"+metric_results['Train Network']+\"_\"+metric_results['Test Network']\n",
    "metric_results = metric_results.sort_values('Model-Train-Test')\n",
    "metric_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include DeepTTE Generalization Results\n",
    "kcm_deeptte_gen_res = data_utils.extract_deeptte_results(\"KCM\", run_folder_tte, \"kcm/\", generalization_flag=True)\n",
    "atb_deeptte_gen_res = data_utils.extract_deeptte_results(\"ATB\", run_folder_tte, \"atb/\", generalization_flag=True)\n",
    "deeptte_gen_res = pd.concat([kcm_deeptte_gen_res, atb_deeptte_gen_res])\n",
    "deeptte_gen_res[\"Model\"] = \"DEEPTTE\"\n",
    "deeptte_gen_res[\"Train Network\"] = deeptte_gen_res[\"City\"]\n",
    "deeptte_gen_res[\"Test Network\"] = deeptte_gen_res[\"Loss Set\"]\n",
    "deeptte_gen_res[\"Metric\"] = deeptte_gen_res[\"RMSE\"]\n",
    "deeptte_gen_res = deeptte_gen_res[[\"Model\",\"Train Network\",\"Test Network\",\"Metric\"]]\n",
    "deeptte_gen_res['Model-Train-Test'] = deeptte_gen_res['Model']+\"_\"+deeptte_gen_res['Train Network']+\"_\"+deeptte_gen_res['Test Network']\n",
    "metric_results = pd.concat([metric_results, deeptte_gen_res])\n",
    "metric_results = metric_results.sort_values('Model-Train-Test')\n",
    "metric_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = metric_results[metric_results['Test Network']==\"KCM\"]\n",
    "fig, axes = plt.subplots(1,1)\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(6)\n",
    "sns.barplot(plot_data, x=\"Metric\", y=\"Model-Train-Test\", hue=\"Model\", dodge=False)\n",
    "axes.set_ylabel(\"Model\")\n",
    "axes.set_xlabel(\"RMSE\")\n",
    "fig.suptitle('Model Generalization Performance On KCM', fontsize=16)\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"../plots/model_generalization.png\", dpi=1800, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = metric_results[metric_results['Test Network']==\"ATB\"]\n",
    "fig, axes = plt.subplots(1,1)\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(6)\n",
    "sns.barplot(plot_data, x=\"Metric\", y=\"Model-Train-Test\", hue=\"Model\", dodge=False)\n",
    "axes.set_ylabel(\"Model\")\n",
    "axes.set_xlabel(\"RMSE\")\n",
    "fig.suptitle('Model Generalization Performance On ATB', fontsize=16)\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"../plots/model_generalization.png\", dpi=1800, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "valle_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b30fe1de1713ca8e7537eef068b13a2de77ded03f86aab2e80ea73416dd3d704"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
